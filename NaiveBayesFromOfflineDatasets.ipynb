{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaiveBayesFromOfflineDatasets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1pyffZANmIOV64TXSsSUwGch8UzYD-3me",
      "authorship_tag": "ABX9TyPAO4mBRqNGHr2wsb2PEUf8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xanasa14/MLImplementations/blob/master/NaiveBayesFromOfflineDatasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRn3egE0WTQp",
        "outputId": "8baeb6b5-28e0-4999-f029-a65029750117"
      },
      "source": [
        "import numpy as np, pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.pipeline import make_pipeline\r\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\r\n",
        "import nltk\r\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
        "nltk.download('wordnet')\r\n",
        "\r\n",
        "\r\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\r\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fPWR6U9WjUj"
      },
      "source": [
        "#importing our cancer dataset\r\n",
        "#dataset = pd.read_csv('/content/Errors.csv')\r\n",
        "#/content/fruitsBi.csv\r\n",
        "\r\n",
        "dataset = pd.read_csv('/content/fruitsBi.csv')\r\n",
        "#X = dataset['radius mean', 'texture mean', 'perimeter mean', 'area_mean','smoothness_mean','compactness mean','concavity mean','concave points mean','symmetry mean','fractal dimension mean','radius se' ]\r\n",
        "col_list = ['id', 'label', 'Description']\r\n",
        "#df = pd.read_csv(\"/content/Errors.csv\", usecols=col_list)#dataset['diagnosis']\r\n",
        "df = pd.read_csv(\"/content/fruitsBi.csv\", usecols=col_list)#dataset['diagnosis']\r\n",
        "#Setting our X and Y for trainig and testing \r\n",
        "\r\n",
        "#Dat  = df['Description']\r\n",
        "#Y = dataset['label']\r\n",
        "#print(type(Dat))\r\n",
        "#print(Dat[0])\r\n",
        "\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjG6ojPhTCn9",
        "outputId": "b324f4b8-51f4-4aa5-e317-39b3b14b7beb"
      },
      "source": [
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\r\n",
        "\r\n",
        "stop_words = stopwords.words('english')\r\n",
        "#print(stopwords.words('english'))\r\n",
        "from nltk.stem import WordNetLemmatizer \r\n",
        "\r\n",
        "import spacy\r\n",
        "\r\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) \r\n",
        "import string  \r\n",
        "\r\n",
        "#Removing punctuationes and those characters\r\n",
        "def remove_punctuations(text):\r\n",
        "  for punctuation in string.punctuation:\r\n",
        "      text = text.replace(punctuation, '')\r\n",
        "  return text\r\n",
        "# Lowering Text from DataFrame\r\n",
        "def loweringText(text):\r\n",
        "    text = text.lower()\r\n",
        "    return text\r\n",
        "def convert(lst): \r\n",
        "    return ([i for item in lst for i in item.split()]) \r\n",
        "def remove_StopWords(text):\r\n",
        "  line = text.split()\r\n",
        "  text = \"\"\r\n",
        "  for word in line:\r\n",
        "    if(word not in stop_words):\r\n",
        "      text += word\r\n",
        "      text += \" \"\r\n",
        "  return text\r\n",
        "\r\n",
        "def lemmatize(text):\r\n",
        "  line = text.split()\r\n",
        "  txt = \"\"\r\n",
        "  for word in line:\r\n",
        "    doc = nlp(word)\r\n",
        "    for token in doc:\r\n",
        "      txt += token.lemma_\r\n",
        "      txt += \" \"\r\n",
        "  return txt\r\n",
        "\r\n",
        "  # Extract the lemma for each token and join\r\n",
        "  #print(line)\r\n",
        "  #> 'the strip bat be hang on -PRON- foot for good'\r\n",
        "  return line \r\n",
        "df[\"Desc\"] = df['Description'].apply(remove_punctuations).apply(loweringText).apply(remove_StopWords).apply(lemmatize)\r\n",
        "#df [\"D\"] = df[\"Desc\"].apply(lemmatize)\r\n",
        "#df.authors.apply(eval).apply(len).sum()\r\n",
        "\r\n",
        "Dat  = df['Desc']\r\n",
        "Y = dataset['label']"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJiZDWV3lQDH",
        "outputId": "177a31df-de97-4296-c387-d04901246794"
      },
      "source": [
        "print(Y)\r\n",
        "print(Dat)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0      apple\n",
            "1      apple\n",
            "2      apple\n",
            "3      apple\n",
            "4      apple\n",
            "5     banana\n",
            "6     banana\n",
            "7     banana\n",
            "8     banana\n",
            "9     banana\n",
            "10    banana\n",
            "11    banana\n",
            "12    banana\n",
            "13     apple\n",
            "14     apple\n",
            "15     apple\n",
            "16     apple\n",
            "17     apple\n",
            "18    banana\n",
            "19    banana\n",
            "20    banana\n",
            "21    banana\n",
            "22    banana\n",
            "23    banana\n",
            "24    banana\n",
            "25    banana\n",
            "26     apple\n",
            "27     apple\n",
            "28     apple\n",
            "29     apple\n",
            "30     apple\n",
            "Name: label, dtype: object\n",
            "0              red \n",
            "1              red \n",
            "2              red \n",
            "3              red \n",
            "4              red \n",
            "5     yellow color \n",
            "6     yellow color \n",
            "7     yellow color \n",
            "8     yellow color \n",
            "9     yellow color \n",
            "10    yellow color \n",
            "11    yellow color \n",
            "12    yellow color \n",
            "13             red \n",
            "14             red \n",
            "15             red \n",
            "16             red \n",
            "17             red \n",
            "18    yellow color \n",
            "19    yellow color \n",
            "20    yellow color \n",
            "21    yellow color \n",
            "22    yellow color \n",
            "23    yellow color \n",
            "24    yellow color \n",
            "25    yellow color \n",
            "26             red \n",
            "27             red \n",
            "28             red \n",
            "29             red \n",
            "30             red \n",
            "Name: Desc, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mqedRrAag1I"
      },
      "source": [
        "#X and y split to train and test\r\n",
        "\r\n",
        "#y = df['label']\r\n",
        "#X = df.drop('label', axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iubsUXtkYpHW"
      },
      "source": [
        "#print(X)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROVGjHd6Y7Pp",
        "outputId": "99fa622c-09f4-4921-c90e-9ca1671ac44b"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(Dat, Y,test_size=0.2, random_state =0  )\r\n",
        "#_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.2, random_state =0  )\r\n",
        "\r\n",
        "print(\"\\nX_train:\\n\")\r\n",
        "print(X_train.head())\r\n",
        "print(X_train.shape)\r\n",
        "\r\n",
        "print(\"\\nX_test:\\n\")\r\n",
        "print(X_test.head())\r\n",
        "print(X_test.shape)\r\n",
        "\r\n",
        "print(\"\\nX_train:\\n\")\r\n",
        "print(y_train.head())\r\n",
        "print(y_train.shape)\r\n",
        "\r\n",
        "print(\"\\nX_test:\\n\")\r\n",
        "print(y_test.head())\r\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "X_train:\n",
            "\n",
            "11    yellow collor\n",
            "17          its red\n",
            "23    yellow collor\n",
            "5     yellow collor\n",
            "16          its red\n",
            "Name: Description, dtype: object\n",
            "(24,)\n",
            "\n",
            "X_test:\n",
            "\n",
            "2           its red\n",
            "29          its red\n",
            "13          its red\n",
            "10    yellow collor\n",
            "27          its red\n",
            "Name: Description, dtype: object\n",
            "(7,)\n",
            "\n",
            "X_train:\n",
            "\n",
            "11    banana\n",
            "17     apple\n",
            "23    banana\n",
            "5     banana\n",
            "16     apple\n",
            "Name: label, dtype: object\n",
            "(24,)\n",
            "\n",
            "X_test:\n",
            "\n",
            "2      apple\n",
            "29     apple\n",
            "13     apple\n",
            "10    banana\n",
            "27     apple\n",
            "Name: label, dtype: object\n",
            "(7,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7G1mHiowa59L",
        "outputId": "f143f98a-c256-4e2c-a8c3-9fc6da1a95df"
      },
      "source": [
        "# Build the model\r\n",
        "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\r\n",
        "# Train the model using the training data\r\n",
        "model.fit(X_train, y_train)\r\n",
        "# Predict the categories of the test data\r\n",
        "predicted_categories = model.predict(Y)\r\n",
        "print(predicted_categories)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['banana' 'banana' 'banana' 'banana' 'banana' 'banana' 'banana' 'banana'\n",
            " 'banana' 'banana' 'banana' 'banana' 'banana' 'banana' 'banana' 'banana'\n",
            " 'banana' 'banana' 'banana' 'banana' 'banana' 'banana' 'banana' 'banana'\n",
            " 'banana' 'banana' 'banana' 'banana' 'banana' 'banana' 'banana']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKbl-nW4e56s",
        "outputId": "ee0d4138-31da-4320-c0c6-f6341719d166"
      },
      "source": [
        "print(predicted_categories)\r\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['banana' 'banana' 'banana' 'banana' 'banana' 'banana' 'banana' 'banana'\n",
            " 'banana' 'banana' 'banana' 'banana' 'banana' 'banana' 'banana' 'banana'\n",
            " 'banana' 'banana' 'banana' 'banana' 'banana' 'banana' 'banana' 'banana'\n",
            " 'banana' 'banana' 'banana' 'banana' 'banana' 'banana' 'banana']\n",
            "0      apple\n",
            "1      apple\n",
            "2      apple\n",
            "3      apple\n",
            "4      apple\n",
            "5     banana\n",
            "6     banana\n",
            "7     banana\n",
            "8     banana\n",
            "9     banana\n",
            "10    banana\n",
            "11    banana\n",
            "12    banana\n",
            "13     apple\n",
            "14     apple\n",
            "15     apple\n",
            "16     apple\n",
            "17     apple\n",
            "18    banana\n",
            "19    banana\n",
            "20    banana\n",
            "21    banana\n",
            "22    banana\n",
            "23    banana\n",
            "24    banana\n",
            "25    banana\n",
            "26     apple\n",
            "27     apple\n",
            "28     apple\n",
            "29     apple\n",
            "30     apple\n",
            "Name: label, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU1oZgkzbm76",
        "outputId": "8b2ee99a-a1e2-470c-8790-c6a304093222"
      },
      "source": [
        "\r\n",
        "print(\"The accuracy is {}\".format(accuracy_score(predicted_categories, Y)))\r\n",
        "#he accuracy is 0.7738980350504514\r\n",
        "#The accuracy is 0.47058823529411764\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is 0.5161290322580645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J26nCKXCc712",
        "outputId": "a6789fc9-75ca-410c-c793-15d082dc8e28"
      },
      "source": [
        "confusion_matrix(Y, predicted_categories)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0, 15],\n",
              "       [ 0, 16]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-FG0XQBgjUk",
        "outputId": "2f35f8bb-e711-4ed1-d447-d0d3a8aca2a5"
      },
      "source": [
        "col_list = ['id', 'label', 'Description']\r\n",
        "df = pd.read_csv(\"/content/Errors.csv\", usecols=col_list)#dataset['diagnosis']\r\n",
        "import nltk\r\n",
        "\r\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\r\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\r\n",
        "\r\n",
        "def lemmatize_text(text):\r\n",
        "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\r\n",
        "\r\n",
        "df['description'] = df[\"Description\"].apply(lemmatize_text)\r\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    id  ...                                     description\n",
            "0    1  ...                 [It, froze, for, a, long, time]\n",
            "1    2  ...                   [A, blue, screen, showed, up]\n",
            "2    3  ...                       [Cant, move, the, cursor]\n",
            "3    4  ...                            [Cant, do, anything]\n",
            "4    5  ...                             [It, say, to, wait]\n",
            "5    6  ...                            [Told, me, to, wait]\n",
            "6    7  ...                       [It, is, not, responding]\n",
            "7    8  ...            [The, application, cant, be, closed]\n",
            "8    9  ...                    [Black, Screen, is, showing]\n",
            "9   10  ...   [Its, been, frozen, for, a, very, long, time]\n",
            "10  11  ...                     [A, blue, screen, appeared]\n",
            "11  12  ...                  [The, cursor, is, not, moving]\n",
            "12  13  ...           [It, doe, not, let, me, do, anything]\n",
            "13  14  ...  [The, operator, said, that, I, have, to, wait]\n",
            "14  15  ...                   [A, window, of, please, wait]\n",
            "15  16  ...                       [Nothing, is, responding]\n",
            "16  17  ...            [The, application, is, not, closing]\n",
            "\n",
            "[17 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "Uh71h3uKp0eN",
        "outputId": "d665cdf3-b211-4b48-c78f-1c9945a795a7"
      },
      "source": [
        "import numpy \r\n",
        "from itertools import product\r\n",
        "\r\n",
        "headline_tokens = []\r\n",
        "def lemmatize(s):\r\n",
        "  print(type(s))\r\n",
        "  for news_headline in s:\r\n",
        "    headline_tokens.append([token.text.lower() for token in spacy_nlp(s) if not token.is_stop])\r\n",
        "    print(headline_tokens)\r\n",
        "\r\n",
        "\r\n",
        "#X = dataset.assign(col_lemma = Dat.col.apply(lambda x: lemmatize(x)))\r\n",
        "\r\n",
        "# import pandas as pd \r\n",
        "import pandas as pd \r\n",
        "  \r\n",
        "# Creating empty series \r\n",
        "xavi = pd.Series() \r\n",
        "  \r\n",
        "\r\n",
        "for i in range(len(Dat)):\r\n",
        "  print(Dat[i])\r\n",
        "  xavi.append(product(Dat[i]))\r\n",
        " \r\n",
        "print(xavi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It froze for a long time \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-116-4b0a29dccb56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mxavi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxavi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, to_append, ignore_index, verify_integrity)\u001b[0m\n\u001b[1;32m   2700\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2701\u001b[0m         return concat(\n\u001b[0;32m-> 2702\u001b[0;31m             \u001b[0mto_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2703\u001b[0m         )\n\u001b[1;32m   2704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0;34m\"only Series and DataFrame objs are valid\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 )\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'itertools.product'>'; only Series and DataFrame objs are valid"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMw_SS1osB1G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}