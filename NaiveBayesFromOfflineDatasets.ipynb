{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaiveBayesFromOfflineDatasets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1pyffZANmIOV64TXSsSUwGch8UzYD-3me",
      "authorship_tag": "ABX9TyMNDv5zPTho6AaX6RPQ9lko",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xanasa14/MLImplementations/blob/master/NaiveBayesFromOfflineDatasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRn3egE0WTQp",
        "outputId": "c75f958f-6939-454f-a620-7a3bfe2a53af"
      },
      "source": [
        "\r\n",
        "\r\n",
        "import numpy as np, pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer \r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer \r\n",
        "\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.pipeline import make_pipeline\r\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\r\n",
        "import nltk\r\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
        "nltk.download('wordnet')\r\n",
        "from sklearn.utils.multiclass import unique_labels\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\r\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fPWR6U9WjUj"
      },
      "source": [
        "#importing our cancer dataset\r\n",
        "#dataset = pd.read_csv('/content/Errors.csv')\r\n",
        "#/content/fruitsBi.csv\r\n",
        "\r\n",
        "dataset = pd.read_csv('/content/fruitsBi.csv')\r\n",
        "#X = dataset['radius mean', 'texture mean', 'perimeter mean', 'area_mean','smoothness_mean','compactness mean','concavity mean','concave points mean','symmetry mean','fractal dimension mean','radius se' ]\r\n",
        "col_list = ['id', 'label', 'Description']\r\n",
        "#df = pd.read_csv(\"/content/Errors.csv\", usecols=col_list)#dataset['diagnosis']\r\n",
        "df = pd.read_csv(\"/content/fruitsBi.csv\", usecols=col_list)#dataset['diagnosis']\r\n",
        "#Setting our X and Y for trainig and testing \r\n",
        "\r\n",
        "#Dat  = df['Description']\r\n",
        "#Y = dataset['label']\r\n",
        "#print(type(Dat))\r\n",
        "#print(Dat[0])\r\n",
        "\r\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjG6ojPhTCn9",
        "outputId": "c419e155-0123-4f2a-dd90-de76b2d15ccd"
      },
      "source": [
        "import spacy\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\r\n",
        "\r\n",
        "stop_words = stopwords.words('english')\r\n",
        "#print(stopwords.words('english'))\r\n",
        "from nltk.stem import WordNetLemmatizer \r\n",
        "\r\n",
        "\r\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) \r\n",
        "import string  \r\n",
        "\r\n",
        "#Removing punctuationes and those characters\r\n",
        "def remove_punctuations(text):\r\n",
        "  for punctuation in string.punctuation:\r\n",
        "      text = text.replace(punctuation, '')\r\n",
        "  return text\r\n",
        "# Lowering Text from DataFrame\r\n",
        "def loweringText(text):\r\n",
        "    text = text.lower()\r\n",
        "    return text\r\n",
        "def convert(lst): \r\n",
        "    return ([i for item in lst for i in item.split()]) \r\n",
        "def remove_StopWords(text):\r\n",
        "  line = text.split()\r\n",
        "  text = \"\"\r\n",
        "  for word in line:\r\n",
        "    if(word not in stop_words):\r\n",
        "      text += word\r\n",
        "      text += \" \"\r\n",
        "  return text\r\n",
        "\r\n",
        "def lemmatize(text):\r\n",
        "  line = text.split()\r\n",
        "  txt = \"\"\r\n",
        "  for word in line:\r\n",
        "    doc = nlp(word)\r\n",
        "    for token in doc:\r\n",
        "      txt += token.lemma_\r\n",
        "      txt += \" \"\r\n",
        "  return txt\r\n",
        "\r\n",
        "  # Extract the lemma for each token and join\r\n",
        "  #print(line)\r\n",
        "  #> 'the strip bat be hang on -PRON- foot for good'\r\n",
        "  return line \r\n",
        "df[\"Desc\"] = df['Description'].apply(remove_punctuations).apply(loweringText).apply(remove_StopWords).apply(lemmatize)\r\n",
        "#df [\"D\"] = df[\"Desc\"].apply(lemmatize)\r\n",
        "#df.authors.apply(eval).apply(len).sum()\r\n",
        "\r\n",
        "Dat  = df['Desc']\r\n",
        "Y = dataset['label']"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_huRIgE3Jqv",
        "outputId": "5a0b1e70-d961-4607-ace6-3b4e4f182a29"
      },
      "source": [
        "cv=CountVectorizer() \r\n",
        " \r\n",
        "# this steps generates word counts for the words in your docs \r\n",
        "word_count_vector=cv.fit_transform(Dat)\r\n",
        "word_count_vector.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8zpzJ953cbF"
      },
      "source": [
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \r\n",
        "tfidf_transformer.fit(word_count_vector)\r\n",
        "# print idf values \r\n",
        "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \r\n",
        " \r\n",
        "# sort ascending \r\n",
        "df_idf.sort_values(by=['idf_weights'])\r\n",
        "\r\n",
        "#Compute the TFIDF score for your documents\r\n",
        "\r\n",
        "# count matrix \r\n",
        "count_vector=cv.transform(Dat) \r\n",
        " \r\n",
        "# tf-idf scores \r\n",
        "tf_idf_vector=tfidf_transformer.transform(count_vector)\r\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "bvugLexv4M9-",
        "outputId": "2f055552-34bd-4d56-b916-f89c8b054733"
      },
      "source": [
        "feature_names = cv.get_feature_names() \r\n",
        " \r\n",
        "#get tfidf vector for first document \r\n",
        "first_document_vector=tf_idf_vector[0]\r\n",
        " \r\n",
        "#print the scores \r\n",
        "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \r\n",
        "df.sort_values(by=[\"tfidf\"],ascending=False)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>red</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>color</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>yellow</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        tfidf\n",
              "red       1.0\n",
              "color     0.0\n",
              "yellow    0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKS46ceu4yf1"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer \r\n",
        " \r\n",
        "# settings that you use for count vectorizer will go here \r\n",
        "tfidf_vectorizer=TfidfVectorizer(use_idf=True) \r\n",
        " \r\n",
        "# just send in all your docs here \r\n",
        "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(Dat)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# get the first vector out (for the first document) \r\n",
        "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \r\n",
        " \r\n",
        "# place tf-idf values in a pandas data frame \r\n",
        "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]), df.sort_values(by=[\"tfidf\"],ascending=False)\r\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWXtB2Y15KYx",
        "outputId": "ff4e33df-2f7f-4266-ee04-9a26d96b1c02"
      },
      "source": [
        "tfidf_vectorizer=TfidfVectorizer(use_idf=True)\r\n",
        " \r\n",
        "# just send in all your docs here\r\n",
        "fitted_vectorizer=tfidf_vectorizer.fit(Dat)\r\n",
        "tfidf_vectorizer_vectors=fitted_vectorizer.transform(Dat)\r\n",
        "print(tfidf_vectorizer_vectors)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 1)\t1.0\n",
            "  (1, 1)\t1.0\n",
            "  (2, 1)\t1.0\n",
            "  (3, 1)\t1.0\n",
            "  (4, 1)\t1.0\n",
            "  (5, 2)\t0.7071067811865476\n",
            "  (5, 0)\t0.7071067811865476\n",
            "  (6, 2)\t0.7071067811865476\n",
            "  (6, 0)\t0.7071067811865476\n",
            "  (7, 2)\t0.7071067811865476\n",
            "  (7, 0)\t0.7071067811865476\n",
            "  (8, 2)\t0.7071067811865476\n",
            "  (8, 0)\t0.7071067811865476\n",
            "  (9, 2)\t0.7071067811865476\n",
            "  (9, 0)\t0.7071067811865476\n",
            "  (10, 2)\t0.7071067811865476\n",
            "  (10, 0)\t0.7071067811865476\n",
            "  (11, 2)\t0.7071067811865476\n",
            "  (11, 0)\t0.7071067811865476\n",
            "  (12, 2)\t0.7071067811865476\n",
            "  (12, 0)\t0.7071067811865476\n",
            "  (13, 1)\t1.0\n",
            "  (14, 1)\t1.0\n",
            "  (15, 1)\t1.0\n",
            "  (16, 1)\t1.0\n",
            "  (17, 1)\t1.0\n",
            "  (18, 2)\t0.7071067811865476\n",
            "  (18, 0)\t0.7071067811865476\n",
            "  (19, 2)\t0.7071067811865476\n",
            "  (19, 0)\t0.7071067811865476\n",
            "  (20, 2)\t0.7071067811865476\n",
            "  (20, 0)\t0.7071067811865476\n",
            "  (21, 2)\t0.7071067811865476\n",
            "  (21, 0)\t0.7071067811865476\n",
            "  (22, 2)\t0.7071067811865476\n",
            "  (22, 0)\t0.7071067811865476\n",
            "  (23, 2)\t0.7071067811865476\n",
            "  (23, 0)\t0.7071067811865476\n",
            "  (24, 2)\t0.7071067811865476\n",
            "  (24, 0)\t0.7071067811865476\n",
            "  (25, 2)\t0.7071067811865476\n",
            "  (25, 0)\t0.7071067811865476\n",
            "  (26, 1)\t1.0\n",
            "  (27, 1)\t1.0\n",
            "  (28, 1)\t1.0\n",
            "  (29, 1)\t1.0\n",
            "  (30, 1)\t1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROVGjHd6Y7Pp",
        "outputId": "3f9c1e5e-9052-4fe9-ff03-393443994b4d"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(Dat, Y,test_size=0.5, random_state =0  )\r\n",
        "#_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.2, random_state =0  )\r\n",
        "\r\n",
        "print(\"\\nX_train:\\n\")\r\n",
        "print(X_train.head())\r\n",
        "print(X_train.shape)\r\n",
        "\r\n",
        "print(\"\\nX_test:\\n\")\r\n",
        "print(X_test.head())\r\n",
        "print(X_test.shape)\r\n",
        "\r\n",
        "print(\"\\nX_train:\\n\")\r\n",
        "print(y_train.head())\r\n",
        "print(y_train.shape)\r\n",
        "\r\n",
        "print(\"\\nX_test:\\n\")\r\n",
        "print(y_test.head())\r\n",
        "print(y_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "X_train:\n",
            "\n",
            "1              red \n",
            "30             red \n",
            "6     yellow color \n",
            "4              red \n",
            "18    yellow color \n",
            "Name: Desc, dtype: object\n",
            "(15,)\n",
            "\n",
            "X_test:\n",
            "\n",
            "2              red \n",
            "29             red \n",
            "13             red \n",
            "10    yellow color \n",
            "27             red \n",
            "Name: Desc, dtype: object\n",
            "(16,)\n",
            "\n",
            "X_train:\n",
            "\n",
            "1      apple\n",
            "30     apple\n",
            "6     banana\n",
            "4      apple\n",
            "18    banana\n",
            "Name: label, dtype: object\n",
            "(15,)\n",
            "\n",
            "X_test:\n",
            "\n",
            "2      apple\n",
            "29     apple\n",
            "13     apple\n",
            "10    banana\n",
            "27     apple\n",
            "Name: label, dtype: object\n",
            "(16,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7G1mHiowa59L",
        "outputId": "ec91e0e6-765f-4385-c9e0-93d333ae4c68"
      },
      "source": [
        "# Build the model\r\n",
        "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\r\n",
        "# Train the model using the training data\r\n",
        "model.fit(X_train, y_train)\r\n",
        "# Predict the categories of the test data\r\n",
        "y_pred = model.predict(X_test)\r\n",
        "print(y_pred)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['apple' 'apple' 'apple' 'banana' 'apple' 'banana' 'banana' 'banana'\n",
            " 'apple' 'banana' 'banana' 'apple' 'banana' 'apple' 'banana' 'banana']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU1oZgkzbm76",
        "outputId": "5a55282c-8be4-43e8-b0f3-cbb14c7cceb6"
      },
      "source": [
        "\r\n",
        "classes = unique_labels(X_test, predicted_categories)\r\n",
        "\r\n",
        "print('classificaiton report 1\\n')\r\n",
        "print(classification_report(y_test,y_pred))\r\n",
        "print(\"The accuracy is {}\".format(accuracy_score(y_test, y_pred)))\r\n",
        "#he accuracy is 0.7738980350504514\r\n",
        "#The accuracy is 0.47058823529411764\r\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classificaiton report 1\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00         7\n",
            "      banana       1.00      1.00      1.00         9\n",
            "\n",
            "    accuracy                           1.00        16\n",
            "   macro avg       1.00      1.00      1.00        16\n",
            "weighted avg       1.00      1.00      1.00        16\n",
            "\n",
            "The accuracy is 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J26nCKXCc712",
        "outputId": "3e335687-0b26-414e-bae6-4c54e6892029"
      },
      "source": [
        "confusion_matrix(y_test, predicted_categories)\r\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[7, 0],\n",
              "       [0, 9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vq3QxZcz6Jm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}