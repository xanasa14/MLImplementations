{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentences_SimilaritiesEspanol.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1dqo5J6ZteKlA6ehv70-xUdOPMZ-3VEoi",
      "authorship_tag": "ABX9TyOEqH+NvMrfw0HhMA0ISjQM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xanasa14/MLImplementations/blob/master/Sentences_SimilaritiesEspanol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyxqD4xkg-Bx"
      },
      "source": [
        "\r\n",
        "Installing neeeded software"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2SFGQapg0SC",
        "outputId": "e3eb71cf-a1fa-4084-bada-a4596d536b60"
      },
      "source": [
        "\r\n",
        "\r\n",
        "!pip install --upgrade gensim\r\n",
        "!pip install -U pip setuptools wheel\r\n",
        "#pip install -U spacy\r\n",
        "\r\n",
        "!python -m spacy download es_core_news_sm\r\n",
        "!spacy download es_core_news_sm\r\n",
        "#!python -m spacy download es_core_news_md\r\n",
        "#!spacy download es_core_news_md\r\n",
        "from gensim.models import Word2Vec\r\n",
        "from gensim.models.keyedvectors import KeyedVectors\r\n",
        "import spacy\r\n",
        "print(spacy.__version__)\r\n",
        "\r\n",
        "nlp = spacy.load('es_core_news_sm')\r\n",
        " \r\n"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.8.3)\n",
            "Collecting gensim\n",
            "  Using cached gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2 MB)\n",
            "  Using cached gensim-3.8.2-cp36-cp36m-manylinux1_x86_64.whl (24.2 MB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (4.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (21.0)\n",
            "Collecting pip\n",
            "  Using cached pip-21.0-py3-none-any.whl (1.5 MB)\n",
            "  Using cached pip-20.3.4-py2.py3-none-any.whl (1.5 MB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (52.0.0)\n",
            "Collecting setuptools\n",
            "  Using cached setuptools-52.0.0-py3-none-any.whl (784 kB)\n",
            "  Using cached setuptools-51.3.3-py3-none-any.whl (786 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (0.36.2)\n",
            "Collecting wheel\n",
            "  Using cached wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
            "  Using cached wheel-0.36.1-py2.py3-none-any.whl (34 kB)\n",
            "Collecting es_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz (16.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.2 MB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (52.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2020.12.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n",
            "Collecting es_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz (16.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.2 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (52.0.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2020.12.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n",
            "2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anhb-qgfhD3P"
      },
      "source": [
        "Loading the model from gensim's vectors, and \r\n",
        "Creating my sentences to be compared\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2ILEqy3AJqu"
      },
      "source": [
        "\r\n",
        "modelo = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/GensimSpanish/sbw_vectors.bin\", binary=True)\r\n",
        "\r\n",
        "\r\n",
        "news_headlines= []\r\n",
        "\r\n",
        "\r\n",
        "news_headline0 = \"Te gustan las empanadas?\"\r\n",
        "news_headline1 = \"Te gustaron las empanadas?\"\r\n",
        "news_headline2 = \"Sopa de pollo es lo mejor que hay\"\r\n",
        "news_headline3 = \"habéis comido las empanadas?\"\r\n",
        "news_headline4 = \"habéis degustado de las empanadas?\"\r\n",
        "news_headline5 = \"Tengo que estudiar mucho para el examen que me agrada\"\r\n",
        "\r\n",
        "news_headlines_withoutLemma = [news_headline0 ,news_headline1,news_headline2, news_headline3,news_headline4,news_headline5]\r\n",
        "#https://unipython.com/curso-de-procesamiento-de-textos-gensim/"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibHnliJMhVSj"
      },
      "source": [
        "Text Processing\r\n",
        "\r\n",
        "Before finding out, we need to do some text process to be consistent with every word; Consquently, we will get rid of punctuations and we will also apply lemmatization using the Spanish version of Spacy\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prTMC1JuBY6a",
        "outputId": "1ab56beb-eb8b-4802-a501-9fa9e813dc2f"
      },
      "source": [
        "# Python program to remove punctuation from a given string \r\n",
        "# Function to remove punctuation \r\n",
        "def Punctuation(string): \r\n",
        "    string= string.replace(\"-PRON-\",\"\")      \r\n",
        "  \r\n",
        "    # punctuation marks \r\n",
        "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~-'''\r\n",
        "  \r\n",
        "    # traverse the given string and if any punctuation \r\n",
        "    # marks occur replace it with null \r\n",
        "    for x in string.lower(): \r\n",
        "        if x in punctuations: \r\n",
        "            string = string.replace(x, \"\") \r\n",
        "  \r\n",
        "    # Print string without punctuation \r\n",
        "    return (string) \r\n",
        "#Lemmatizing from beginning \r\n",
        "for headlines in news_headlines_withoutLemma:\r\n",
        "   doc = nlp(Punctuation(headlines))\r\n",
        "   line = \" \".join([token.lemma_ for token in doc])\r\n",
        "   line= line.replace(\"-PRON-\",\"\") \r\n",
        "   line= line.replace(\"  \",\" \")      \r\n",
        "     \r\n",
        "\r\n",
        "   print(line)\r\n",
        "   news_headlines.append(line)\r\n",
        "print(news_headlines)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Te gustar los empanar\n",
            "Te gustar los empanar\n",
            "Sopa de pollo ser el mejor que haber\n",
            "haber comer los empanar\n",
            "haber degustar de los empanar\n",
            "Tengo que estudiar mucho parir el examen que me agradar\n",
            "['Te gustar los empanar', 'Te gustar los empanar', 'Sopa de pollo ser el mejor que haber', 'haber comer los empanar', 'haber degustar de los empanar', 'Tengo que estudiar mucho parir el examen que me agradar']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwZlMbNahvLX"
      },
      "source": [
        "Remove Stopwords:\r\n",
        "\r\n",
        "Some words are not as impactful in a sentece as others. We call this removing stopwords from our sentences\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xml13bpjEIid",
        "outputId": "a39c05f1-e4aa-49b3-c785-8a4e6254986a"
      },
      "source": [
        "headline_tokens = []\r\n",
        "for news_headline in news_headlines:\r\n",
        "    headline_tokens.append([token.text.lower() for token in nlp(news_headline) if not token.is_stop])\r\n",
        "\r\n",
        "print(headline_tokens)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['gustar', 'empanar'], ['gustar', 'empanar'], ['sopa', 'pollo'], ['comer', 'empanar'], ['degustar', 'empanar'], ['estudiar', 'parir', 'examen', 'agradar']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j7Utn1eh9Ik"
      },
      "source": [
        "We will apply gensim to see how distant are these sentences, or bag of words\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YWUxt2EB-Xd",
        "outputId": "a1594e1d-b480-473e-f825-1c8850bf97f0"
      },
      "source": [
        "subject_headline = news_headlines[0]\r\n",
        "subject_token = headline_tokens[0]\r\n",
        "\r\n",
        "print('Headline: ', subject_headline)\r\n",
        "print('=' * 50)\r\n",
        "print()\r\n",
        "lineNumber = 0\r\n",
        "for token, headline in zip(headline_tokens, news_headlines):\r\n",
        "    print('-' * 50)\r\n",
        "    print(token)\r\n",
        "    print('Comparing to:', news_headlines_withoutLemma[lineNumber])\r\n",
        "    lineNumber += 1\r\n",
        "    distance = modelo.wmdistance(subject_token, token)\r\n",
        "    print('distance = %.4f' % distance)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Headline:  Te gustar los empanar\n",
            "==================================================\n",
            "\n",
            "--------------------------------------------------\n",
            "['gustar', 'empanar']\n",
            "Comparing to: Te gustan las empanadas?\n",
            "distance = 0.0000\n",
            "--------------------------------------------------\n",
            "['gustar', 'empanar']\n",
            "Comparing to: Te gustaron las empanadas?\n",
            "distance = 0.0000\n",
            "--------------------------------------------------\n",
            "['sopa', 'pollo']\n",
            "Comparing to: Sopa de pollo es lo mejor que hay\n",
            "distance = 4.0874\n",
            "--------------------------------------------------\n",
            "['comer', 'empanar']\n",
            "Comparing to: habéis comido las empanadas?\n",
            "distance = 1.8441\n",
            "--------------------------------------------------\n",
            "['degustar', 'empanar']\n",
            "Comparing to: habéis degustado de las empanadas?\n",
            "distance = 2.3674\n",
            "--------------------------------------------------\n",
            "['estudiar', 'parir', 'examen']\n",
            "Comparing to: Tengo que estudiar mucho para el examen\n",
            "distance = 4.1222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzbWvMHGiLh0"
      },
      "source": [
        "Creating a function that does the gensim similarity and also using it to find some close word to it, word by word. I also use lemmatization on the close words \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAuVyNdaRXiM",
        "outputId": "36a452f2-9a07-411a-edf7-e889dfd5205f"
      },
      "source": [
        "#points\r\n",
        "print(news_headline0)\r\n",
        "print(news_headline4)\r\n",
        "print(headline_tokens[0])\r\n",
        "print(headline_tokens[4])\r\n",
        "def get_Gensim_Sim(sentence1, sentence2):\r\n",
        "  points = 0 \r\n",
        "  for word0 in sentence1:\r\n",
        "    for word1 in sentence2:\r\n",
        "      if (word0 == word1):\r\n",
        "        points +=1\r\n",
        "        break \r\n",
        "      else:\r\n",
        "        word = word0\r\n",
        "        sim = modelo.most_similar(positive=[word1],topn=30)\r\n",
        "        lemmatized = \"\" \r\n",
        "        for i in range(len(sim)):\r\n",
        "          doc = nlp(sim[i][0])\r\n",
        "          lemmatized = \" \".join([token.lemma_ for token in doc])\r\n",
        "          #print(lemmatized, sim[i][0])\r\n",
        "          if (word == lemmatized):\r\n",
        "            \r\n",
        "            print(word, lemmatized)\r\n",
        "            print(\"it is here\")\r\n",
        "            points += 0.7\r\n",
        "            break\r\n",
        "           \r\n",
        "\r\n",
        "\r\n",
        "  score = points/len(headline_tokens[0])\r\n",
        "  print(str(score * 100) + \"% similar\")\r\n",
        "\r\n",
        "\r\n",
        "get_Gensim_Sim(headline_tokens[0],headline_tokens[4])\r\n",
        "\r\n"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Te gustan las empanadas?\n",
            "habéis degustado de las empanadas?\n",
            "['gustar', 'empanar']\n",
            "['degustar', 'empanar']\n",
            "empanar empanar\n",
            "it is here\n",
            "85.0% similar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR0H504UW_xh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}