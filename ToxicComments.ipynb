{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ToxicComments.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1_sqDV99oWum2XDB-5cZvOPDbCiqNUJH1",
      "authorship_tag": "ABX9TyMj0daq2cR9YLmct7vJMitl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xanasa14/MLImplementations/blob/master/ToxicComments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0SFzyZpYcm6"
      },
      "source": [
        "from numpy import array\r\n",
        "from keras.preprocessing.text import one_hot\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers.core import Activation, Dropout, Dense\r\n",
        "from keras.layers import Flatten, LSTM\r\n",
        "from keras.layers import GlobalMaxPooling1D\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.layers import Input\r\n",
        "from keras.layers.merge import Concatenate\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import nltk \r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "#98.08%"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsuFB3oKYcir"
      },
      "source": [
        "\r\n",
        "#toxic_comments = pd.read_csv(\"/content/train.csv\",engine='python',encoding='utf-8')\r\n",
        "toxic_comments = pd.read_csv(\"/content/drive/MyDrive/toxicCommentz/train.csv\")\r\n",
        "\r\n",
        "print(toxic_comments.head())\r\n",
        "\r\n",
        "print(toxic_comments.tail)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnw6Z4ZKX0Kj"
      },
      "source": [
        "print(toxic_comments['comment_text'].head())\r\n",
        "print(toxic_comments.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo8Ec-uuZNFt"
      },
      "source": [
        "filter = toxic_comments[\"comment_text\"] != \"\"\r\n",
        "toxic_comments = toxic_comments[filter]\r\n",
        "toxic_comments = toxic_comments.dropna()\r\n",
        "print(toxic_comments[\"comment_text\"][168])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Yhbu89L286z"
      },
      "source": [
        "counter = 0\r\n",
        "offensive = []\r\n",
        "nonOffensive = []\r\n",
        "for i in range(len(toxic_comments)):\r\n",
        "  #if (i <= 500):\r\n",
        "  if (int(toxic_comments[\"toxic\"][i]) == 1 or int(toxic_comments[\"severe_toxic\"][i]) == 1 or \r\n",
        "      int(toxic_comments[\"obscene\"][i]) == 1 or int(toxic_comments[\"threat\"][i]) == 1 or\r\n",
        "      int(toxic_comments[\"insult\"][i]) == 1 or int(toxic_comments[\"identity_hate\"][i]) == 1):\r\n",
        "    #print(\"Toxic:\" + str(toxic_comments[\"toxic\"][i]))\r\n",
        "    #print(\"Severe_toxic:\" + str(toxic_comments[\"severe_toxic\"][i]))\r\n",
        "    #print(\"Obscene:\" + str(toxic_comments[\"obscene\"][i]))\r\n",
        "    #print(\"Threat:\" + str(toxic_comments[\"threat\"][i]))\r\n",
        "    #print(\"Insult:\" + str(toxic_comments[\"insult\"][i]))\r\n",
        "    #print(\"Identity_hate:\" + str(toxic_comments[\"identity_hate\"][i]))\r\n",
        "    offensive.append(1)\r\n",
        "    nonOffensive.append(0)\r\n",
        "  else: \r\n",
        "    nonOffensive.append(1)\r\n",
        "    offensive.append(0)\r\n",
        "\r\n",
        "# Declare a list that is to be converted into a column \r\n",
        "\r\n",
        "  \r\n",
        "# Using 'Address' as the column name \r\n",
        "# and equating it to the list \r\n",
        "toxic_comments['offensive'] = offensive\r\n",
        "toxic_comments['nonOffensive'] = nonOffensive\r\n",
        "\r\n",
        "\r\n",
        "#toxic_comments = toxic_comments[toxic_comments.offensive == 1]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbHflVeya3y4"
      },
      "source": [
        "\r\n",
        "\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][96]))\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][960]))\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][9600]))\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][96000]))\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][77]))\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][777]))\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][7777]))\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][123]))\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][1234]))\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][12345]))\r\n",
        "\r\n",
        "print(\"Toxic:\" + str(toxic_comments[\"toxic\"][0]))\r\n",
        "print(\"Severe_toxic:\" + str(toxic_comments[\"severe_toxic\"][0]))\r\n",
        "print(\"Obscene:\" + str(toxic_comments[\"obscene\"][0]))\r\n",
        "print(\"Threat:\" + str(toxic_comments[\"threat\"][0]))\r\n",
        "print(\"Insult:\" + str(toxic_comments[\"insult\"][0]))\r\n",
        "print(\"Identity_hate:\" + str(toxic_comments[\"identity_hate\"][0]))\r\n",
        "\r\n",
        "\r\n",
        "len(toxic_comments)\r\n",
        "print(\"Offensive :\" + str (toxic_comments['offensive'][0]))\r\n",
        "print(\"Non-offensive :\" + str (toxic_comments['nonOffensive'][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr1jrdxra5LP"
      },
      "source": [
        "#toxic_comments_labels = toxic_comments[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]]\r\n",
        "toxic_comments_labels = toxic_comments[[\"offensive\", \"nonOffensive\"]]\r\n",
        "\r\n",
        "toxic_comments_OffensiveOrNot = toxic_comments[[\"comment_text\",\"offensive\", \"nonOffensive\"]]\r\n",
        "print(toxic_comments_OffensiveOrNot[\"offensive\"][168])\r\n",
        "print(toxic_comments_OffensiveOrNot[\"comment_text\"][168])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u96GFxTPbAW1"
      },
      "source": [
        "fig_size = plt.rcParams[\"figure.figsize\"]\r\n",
        "fig_size[0] = 10\r\n",
        "fig_size[1] = 8\r\n",
        "plt.rcParams[\"figure.figsize\"] = fig_size\r\n",
        "\r\n",
        "toxic_comments_labels.sum(axis=0).plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEiKGG2gx7M3"
      },
      "source": [
        "\r\n",
        "import spacy\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\r\n",
        "stop_words = stopwords.words('english')\r\n",
        "from nltk.stem import WordNetLemmatizer \r\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) \r\n",
        "import string  \r\n",
        "def loweringText(text):\r\n",
        "    text = text.lower()\r\n",
        "    return text\r\n",
        "#Removing punctuationes and those characters\r\n",
        "def remove_punctuations(text):\r\n",
        "  for punctuation in string.punctuation:\r\n",
        "      text = text.replace(punctuation, '')\r\n",
        "  return text\r\n",
        "# Lowering Text from DataFrame\r\n",
        "\r\n",
        "#Remove stopwords : words that do not put as much significance or impact as other words in the description\r\n",
        "def remove_StopWords(text):\r\n",
        "  line = text.split()\r\n",
        "  text = \"\"\r\n",
        "  for word in line:\r\n",
        "    if(word not in stop_words):\r\n",
        "      text += word\r\n",
        "      text += \" \"\r\n",
        "  return text\r\n",
        "#Implementing lemmatization words by word\r\n",
        "def lemmatize(text):\r\n",
        "  line = text.split()\r\n",
        "  txt = \"\"\r\n",
        "  for word in line:\r\n",
        "    doc = nlp(word)\r\n",
        "    for token in doc:\r\n",
        "      txt += token.lemma_\r\n",
        "      txt += \" \"\r\n",
        "  return txt\r\n",
        "import re\r\n",
        "\r\n",
        "def remove_abb(text):\r\n",
        "  for line in text:\r\n",
        "    #Either use:\r\n",
        "    text = re.sub('http://\\S+|https://\\S+', '', text)\r\n",
        "    text = re.sub('http[s]?://\\S+', '', text)\r\n",
        "\r\n",
        "    text = text.replace(\"he's\", \"he is\")\r\n",
        "    text = text.replace(\"there's\", \"there is\")\r\n",
        "    text = text.replace(\"we're\", \"we are\")\r\n",
        "    text = text.replace(\"that's\", \"that is\")\r\n",
        "    text = text.replace(\"won't\", \"will not\")\r\n",
        "    text = text.replace(\"they're\", \"they are\")\r\n",
        "    text = text.replace(\"can't\", \"cannot\")\r\n",
        "    text = text.replace(\"wasn't\", \"was not\")\r\n",
        "    text = text.replace(\"don\\x89Ûªt\", \"do not\")\r\n",
        "    text = text.replace(\"aren't\", \"are not\")\r\n",
        "    text = text.replace(\"isn't\", \"is not\")\r\n",
        "    text = text.replace(\"what's\", \"what is\")\r\n",
        "    text = text.replace(\"haven't\", \"have not\")\r\n",
        "    text = text.replace(\"hasn't\", \"has not\")\r\n",
        "    text = text.replace(\"there's\", \"there is\")\r\n",
        "    text = text.replace(\"he's\", \"he is\")\r\n",
        "    text = text.replace(\"it's\", \"it is\")\r\n",
        "    text = text.replace(\"you're\", \"you are\")\r\n",
        "    text = text.replace(\"i'm\", \"i am\")\r\n",
        "    text = text.replace(\"shouldn't\", \"should not\")\r\n",
        "    text = text.replace(\"wouldn't\", \"would not\")\r\n",
        "    text = text.replace(\"I\\x89Ûªm\", \"i am\")\r\n",
        "    text = text.replace(\"here's\", \"here is\")\r\n",
        "    text = text.replace(\"you've\", \"you have\")\r\n",
        "    text = text.replace(\"you\\x89Ûªve\", \"you have\")\r\n",
        "    text = text.replace(\"couldn't\", \"could not\")\r\n",
        "    text = text.replace(\"we've\", \"we have\")\r\n",
        "    text = text.replace(\"it\\x89Ûªs\", \"it is\")\r\n",
        "    text = text.replace(\"doesn\\x89Ûªt\", \"does not\")\r\n",
        "    text = text.replace(\"it\\x89Ûªs\", \"it is\")\r\n",
        "    text = text.replace(\"here\\x89Ûªs\", \"here is\")\r\n",
        "    text = text.replace(\".jpeg\", \" \")\r\n",
        "    text = text.replace(\".jpg\", \" \")\r\n",
        "    text = text.replace(\".png\", \" \")\r\n",
        "    text = text.replace(\"'s\", \" is \")\r\n",
        "    text = text.replace(\"→\", \" is \")  \r\n",
        "    text = text.replace(\"\\n\", \" \")\r\n",
        "    text = text.replace(\"trácht\", \" \")\r\n",
        "    text = text.replace(\"didn't\", \"did not\")\r\n",
        "    text = text.replace(\"who's\", \"who is\")\r\n",
        "    text = text.replace(\"i\\x89Ûªve\", \"i have\")\r\n",
        "    text = text.replace(\"y'all\", \"you all\")\r\n",
        "    text = text.replace(\"can\\x89Ûªt\", \"cannot\")\r\n",
        "    text = text.replace(\"would've\", \"would have\")\r\n",
        "    text = text.replace(\"it'll\", \"it will\")\r\n",
        "    text = text.replace(\"we'll\", \"we will\")\r\n",
        "    text = text.replace(\"wouldn\\x89Ûªt\", \"would not\")\r\n",
        "    text = text.replace(\"we've\", \"we have\")\r\n",
        "    text = text.replace(\"he'll\", \"he will\")\r\n",
        "    text = text.replace(\"weren't\", \"were not\")\r\n",
        "    text = text.replace(\"they'll\", \"they will\")\r\n",
        "    text = text.replace(\"they'd\", \"they would\")\r\n",
        "    text = text.replace(\"that\\x89Ûªs\", \"that is\")\r\n",
        "    text = text.replace(\"they've\", \"they have\")\r\n",
        "    text = text.replace(\"i'd\", \"i would\")\r\n",
        "    text = text.replace(\"donå«t\", \"do not\")\r\n",
        "    text = text.replace(\"should've\", \"should have\")\r\n",
        "    text = text.replace(\"you\\x89Ûªre\", \"you are\")\r\n",
        "    text = text.replace(\"where's\", \"where is\")\r\n",
        "    text = text.replace(\"don\\x89Ûªt\", \"do not\")\r\n",
        "    text = text.replace(\"we'd\", \"we would\")\r\n",
        "    text = text.replace(\"i'll\", \"i will\")\r\n",
        "    text = text.replace(\"weren't\", \"were not\")\r\n",
        "    text = text.replace(\"can\\x89Ûªt\", \"cannot\")\r\n",
        "    text = text.replace(\"you\\x89Ûªll\", \"you will\")\r\n",
        "    text = text.replace(\"I\\x89Ûªd\", \"i would\")\r\n",
        "    text = text.replace(\"let's\", \"let us\")\r\n",
        "    text = text.replace(\"it's\", \"it is\")\r\n",
        "    text = text.replace(\"don't\", \"do not\")\r\n",
        "    text = text.replace(\"you're\", \"you are\")\r\n",
        "    text = text.replace(\"i've\", \"i have\")\r\n",
        "    text = text.replace(\"ain't\", \"am not\")\r\n",
        "    text = text.replace(\"doesn't\", \"does not\")\r\n",
        "    text = text.replace(\"i'd\", \"i would\")\r\n",
        "    text = text.replace(\"didn't\", \"did not\")\r\n",
        "    text = text.replace(\"you'll\", \"you will\")\r\n",
        "    text = text.replace(\"let's\", \"let us\")\r\n",
        "    text = text.replace(\"youve\", \"you have\")\r\n",
        "    text = text.replace(\"could've\", \"could have\")\r\n",
        "    text = text.replace(\"Haven't\", \"have not\")\r\n",
        "    text = text.replace(\"you'd\", \"you would\")\r\n",
        "    text = text.replace(\"it's\", \"it is\")\r\n",
        "    text = text.replace(\"「\", \" \")\r\n",
        "    text = text.replace(\"」\", \" \")\r\n",
        "    text = text.replace(\"¤\", \" \")\r\n",
        "    text = text.replace(\"¢\", \" \")\r\n",
        "    text = text.replace(\" – \", \" \")\r\n",
        "    text = text.replace(\" a \", \" \")\r\n",
        "    text = text.replace(\" b \", \" \")\r\n",
        "    text = text.replace(\" c \", \" \")\r\n",
        "    text = text.replace(\" d \", \" \")\r\n",
        "    text = text.replace(\" e \", \" \")\r\n",
        "    text = text.replace(\" f \", \" \")\r\n",
        "    text = text.replace(\" g \", \" \")\r\n",
        "    text = text.replace(\" h \", \" \")\r\n",
        "    text = text.replace(\" i \", \" \")\r\n",
        "    text = text.replace(\" j \", \" \")\r\n",
        "    text = text.replace(\" k \", \" \")\r\n",
        "    text = text.replace(\" l \", \" \")\r\n",
        "    text = text.replace(\" m \", \" \")\r\n",
        "    text = text.replace(\" n \", \" \")\r\n",
        "    text = text.replace(\" o \", \" \")\r\n",
        "    text = text.replace(\" p \", \" \")\r\n",
        "    text = text.replace(\" q \", \" \")\r\n",
        "    text = text.replace(\" r \", \" \")\r\n",
        "    text = text.replace(\" s \", \" \")\r\n",
        "    text = text.replace(\" t \", \" \")\r\n",
        "    text = text.replace(\" u \", \" \")\r\n",
        "    text = text.replace(\" v \", \" \")\r\n",
        "    text = text.replace(\" w \", \" \")\r\n",
        "    text = text.replace(\" x \", \" \")\r\n",
        "    text = text.replace(\" y \", \" \")\r\n",
        "    text = text.replace(\" z \", \" \")\r\n",
        "\r\n",
        "    text = text.replace(\"@\", \" \")\r\n",
        "    text = text.replace(\"?\", \" \")\r\n",
        "    text = text.replace(\"!\",\" \")\r\n",
        "    text = text.replace(\"\\\"\",\"\")\r\n",
        "    text = text.replace(\"(\",\"\")\r\n",
        "    text = text.replace(\")\",\"\")\r\n",
        "    text = text.replace(\"+\", \" \") \r\n",
        "    text = text.replace(\"#\", \" \")\r\n",
        "    text = text.replace(\"}\",\" \")\r\n",
        "    text = text.replace(\"{\",\" \")\r\n",
        "    text = text.replace(\"&\",\" \")\r\n",
        "    text = text.replace(\";\",\" \")\r\n",
        "    text = text.replace(\"%\",\" \")\r\n",
        "    text = text.replace(\":\", \" \")\r\n",
        "    text = text.replace(\"$\",\" \")\r\n",
        "    text = text.replace(\"|\", \" \")\r\n",
        "    text = text.replace(\".\", \" \")\r\n",
        "    text = text.replace(\"·\",\" \")\r\n",
        "    text = text.replace(\",\",\" \")\r\n",
        "    text = text.replace(\" -\",\" \")\r\n",
        "    text = text.replace(\"~\",\" \")\r\n",
        "    text = text.replace(\"1\", \" \") \r\n",
        "    text = text.replace(\"2\", \" \")\r\n",
        "    text = text.replace(\"3\", \" \")\r\n",
        "    text = text.replace(\"4\", \" \")\r\n",
        "    text = text.replace(\"5\",\" \")\r\n",
        "    text = text.replace(\"6\",\" \") \r\n",
        "    text = text.replace(\"7\", \" \")\r\n",
        "    text = text.replace(\"8\", \" \")\r\n",
        "    text = text.replace(\"9\",\" \")\r\n",
        "    text = text.replace(\"0\", \" \")\r\n",
        "    text = text.replace(\"%\", \" \")\r\n",
        "    text = text.replace(\"'\", \" \")\r\n",
        "    text = text.replace(\"/\", \" \")\r\n",
        "    text = text.replace(\"<\", \" \")\r\n",
        "    text = text.replace(\">\", \" \")\r\n",
        "    text = text.replace(\"[\", \" \")\r\n",
        "    text = text.replace(\"]\", \" \")\r\n",
        "    text = text.replace(\"_\",\" \")\r\n",
        "    text = text.replace(\"^\",\" \")\r\n",
        "    text = text.replace(\"`\",\" \")\r\n",
        "    text = text.replace(\"=\",\" \")\r\n",
        "    text = text.replace(\"\\\\\",\" \")\r\n",
        "    text = text.replace(\"*\", \" \")\r\n",
        "    text = text.replace(\"- \", \" \")\r\n",
        "    text = text.replace(\"•\", \" \")\r\n",
        "    \r\n",
        "    text = text.replace(\"   \", \" \")\r\n",
        "    text = text.replace(\"  \", \" \")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    \r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "    return text\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgVWk3eW6H2X"
      },
      "source": [
        "#toxic_comments[\"comment_text_X\"] = toxic_comments['comment_text'].apply(remove_punctuations).apply(loweringText).apply(remove_StopWords).apply(lemmatize)\r\n",
        "#FIRST LOWER CASE EVERYTHING \r\n",
        "toxic_comments[\"comment_text\"] = toxic_comments['comment_text'].apply(loweringText)\r\n",
        "print(\"Lowering the Text \")\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCmMISDeBqAt"
      },
      "source": [
        "\r\n",
        "toxic_comments[\"comment_text\"] = toxic_comments['comment_text'].apply(remove_abb)\r\n",
        "print(\"removed remove_abb \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5BnkuBrCCz4"
      },
      "source": [
        "toxic_comments[\"comment_text\"] = toxic_comments['comment_text'].apply(remove_StopWords)\r\n",
        "print(\"removed remove_StopWords \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oztVMIXaUMf"
      },
      "source": [
        "toxic_comments[\"comment_text\"] = toxic_comments['comment_text'].apply(lemmatize)\r\n",
        "print(\"removed lemmatize \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vzpBQMNbEfI"
      },
      "source": [
        "def preprocess_text(sen):\r\n",
        "    # Remove punctuations and numbers\r\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\r\n",
        "\r\n",
        "    # Single character removal\r\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\r\n",
        "\r\n",
        "    # Removing multiple spaces\r\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\r\n",
        "\r\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOoGpkabbHvz"
      },
      "source": [
        "X = []\r\n",
        "#toxic_comments_labels = toxic_comments[[\"offensive\"]]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "sentences = list(toxic_comments[\"comment_text\"])\r\n",
        "for sen in sentences:\r\n",
        "    X.append(preprocess_text(sen))\r\n",
        "\r\n",
        "y = toxic_comments_labels.values\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQwBQF3xvgRF"
      },
      "source": [
        "print(sentences [96])\r\n",
        "print(sentences [960])\r\n",
        "print(sentences [9600])\r\n",
        "print(sentences [96000])\r\n",
        "print(sentences [777])\r\n",
        "print(sentences [77])\r\n",
        "print(sentences [555])\r\n",
        "print(sentences [666])\r\n",
        "print(sentences [888])\r\n",
        "print(sentences [999])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj1-4XcubJIX"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5tEPYFbRcM9"
      },
      "source": [
        "print(X_train[0])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_U5wPK_bU8z"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\r\n",
        "tokenizer.fit_on_texts(X_train)\r\n",
        "\r\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\r\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\r\n",
        "\r\n",
        "vocab_size = len(tokenizer.word_index) + 1\r\n",
        "\r\n",
        "maxlen = 200\r\n",
        "\r\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\r\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTBFJK6lbWoD"
      },
      "source": [
        "from numpy import array\r\n",
        "from numpy import asarray\r\n",
        "from numpy import zeros\r\n",
        "\r\n",
        "embeddings_dictionary = dict()\r\n",
        "\r\n",
        "#glove_file = open('/content/drive/My Drive/Colab Datasets/glove.6B.100d.txt', encoding=\"utf8\")\r\n",
        "glove_file = open('/content/drive/MyDrive/gloves/glove.6B.100d.txt', encoding=\"utf8\")\r\n",
        "\r\n",
        "for line in glove_file:\r\n",
        "    records = line.split()\r\n",
        "    word = records[0]\r\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\r\n",
        "    embeddings_dictionary[word] = vector_dimensions\r\n",
        "glove_file.close()\r\n",
        "\r\n",
        "embedding_matrix = zeros((vocab_size, 100))\r\n",
        "for word, index in tokenizer.word_index.items():\r\n",
        "    embedding_vector = embeddings_dictionary.get(word)\r\n",
        "    if embedding_vector is not None:\r\n",
        "        embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fi8MCl7bg7q"
      },
      "source": [
        "deep_inputs = Input(shape=(maxlen,))\r\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(deep_inputs)\r\n",
        "LSTM_Layer_1 = LSTM(128)(embedding_layer)\r\n",
        "dense_layer_1 = Dense(2, activation='sigmoid')(LSTM_Layer_1)\r\n",
        "model = Model(inputs=deep_inputs, outputs=dense_layer_1)\r\n",
        "\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phe5qujWcSFX"
      },
      "source": [
        "print(model.summary())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc3z61lFcV-n"
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1, validation_split=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSyq9ZT-cZ9E"
      },
      "source": [
        "score = history.model.evaluate(X_test, y_test, verbose=1)\r\n",
        "\r\n",
        "print(\"Test Score:\", score[0])\r\n",
        "print(\"Test Accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV-YwGi_CLew"
      },
      "source": [
        "\r\n",
        "print(X_test.shape)\r\n",
        "print(y_test.shape)\r\n",
        "\r\n",
        "print(score)\r\n",
        "toxic_comments_labels.values\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKzGt7nCqIMJ"
      },
      "source": [
        "history.model.predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY8LpPC-l2fm"
      },
      "source": [
        "model.save('/content/drive/MyDrive/toxicCommentz/my_model.h5')\r\n",
        "# LOAD \r\n",
        "import keras\r\n",
        "model = keras.models.load_model('/content/drive/MyDrive/toxicCommentz/my_model.h5')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pX5APvWtKew"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlDNgxxIB62I"
      },
      "source": [
        "y_pred = history.model.predict(X_test)\r\n",
        "print(y_pred)\r\n",
        "print(y_test)\r\n",
        "print(y_pred.shape)\r\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt2HI-xdaOa_"
      },
      "source": [
        "print(len(y_pred))\r\n",
        "print(len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoXUNjG3p_q_"
      },
      "source": [
        "\r\n",
        "\r\n",
        "import sklearn.metrics as metrics\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "target_names= [\"Offensive\" , \"NonOffensive\"]\r\n",
        "print(classification_report(np.argmax(y_pred, axis=1), np.argmax(y_test, axis=1), target_names=target_names))\r\n",
        "\r\n",
        "\r\n",
        "matrix = metrics.confusion_matrix(y_pred.argmax(axis=1), y_test.argmax(axis=1),)\r\n",
        "# 151\r\n",
        "# 63678\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n2-36qn-uRJ"
      },
      "source": [
        "matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d6SFcbB_TPW"
      },
      "source": [
        "print(y_pred.shape)\r\n",
        "y_pred2 = y_pred\r\n",
        "y_test2 = y_test\r\n",
        "\r\n",
        "Size = len (y_test2)\r\n",
        "\r\n",
        "y_test2 = y_test2.reshape((Size,-1))\r\n",
        "y_pred2 = y_pred2.reshape((Size,-1))\r\n",
        "\r\n",
        "\r\n",
        "print(y_pred2.shape)\r\n",
        "print(y_test2.shape)\r\n",
        "\r\n",
        "matrix = metrics.confusion_matrix(y_pred2.argmax(axis=1), y_test2.argmax(axis=1))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se5Hr8L5Wlm8"
      },
      "source": [
        "matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCx6dvYxNme0"
      },
      "source": [
        "CR = metrics.accuracy_score(y_pred2.argmax(axis=1), y_test2.argmax(axis=1))\r\n",
        "print(CR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEVGvfqrEg3V"
      },
      "source": [
        "#WIthout Lemma and Abba\r\n",
        "             precision    recall  f1-score   support\r\n",
        "\r\n",
        "   Offensive       0.01      0.90      0.02        80\r\n",
        "NonOffensive       1.00      0.90      0.95     63749\r\n",
        "\r\n",
        "    accuracy                           0.90     63829\r\n",
        "   macro avg       0.51      0.90      0.48     63829\r\n",
        "weighted avg       1.00      0.90      0.95     63829\r\n",
        "\r\n",
        "\r\n",
        "0.8984474141847749\r\n",
        "array([[   72,     8],\r\n",
        "       [ 6474, 57275]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG9gDiRytYBa"
      },
      "source": [
        "              precision    recall  f1-score   support\r\n",
        "\r\n",
        "   Offensive       0.57      0.92      0.70      4032\r\n",
        "NonOffensive       0.99      0.95      0.97     59797\r\n",
        "\r\n",
        "    accuracy                           0.95     63829\r\n",
        "   macro avg       0.78      0.94      0.84     63829\r\n",
        "weighted avg       0.97      0.95      0.96     63829\r\n",
        "\r\n",
        "0.9509313948205361\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu5o4_-s3vZY"
      },
      "source": [
        "array([[ 3079,   378],\r\n",
        "       [ 3467, 56905]])\r\n",
        "             precision    recall  f1-score   support\r\n",
        "\r\n",
        "   Offensive       0.47      0.89      0.62      3457\r\n",
        "NonOffensive       0.99      0.94      0.97     60372\r\n",
        "\r\n",
        "    accuracy                           0.94     63829\r\n",
        "   macro avg       0.73      0.92      0.79     63829\r\n",
        "weighted avg       0.97      0.94      0.95     63829"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0R7ynutsGSt"
      },
      "source": [
        "strin = \"Link to http://www.langandlit.ualberta.ca/Fall2004/SteigelBainbridge.html mentions this a bit - he stood as an election candidate for Respect.\" \r\n",
        "strin = strin.replace(\"http:\\/\\/[A-Za-z0-9.\\/]+html\",\"\")\r\n",
        "strin\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEQC-zir9yJL"
      },
      "source": [
        "#Define the text from which you want to replace the url with \"\".\r\n",
        "text ='''The link to this post is http://www.langandlit.ualberta.ca/Fall2004/SteigelBainbridge.html'''\r\n",
        "\r\n",
        "import re\r\n",
        "#Either use:\r\n",
        "re.sub('http://\\S+|https://\\S+', '', text)\r\n",
        "#OR \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzWRfxsbM8m4"
      },
      "source": [
        "test10 = \"gift cards downloads december possible use gift cards purchase mp even though gift card faq claims yes amazon com gift cards used buy amazon mp unbox downloads strictly speaking false statement able pay orders gift card time one person already removed line reverted good way go providing verification \"\r\n",
        "#PREDICTING ONE \r\n",
        "test11 = \"suck my dick you cunt\"\r\n",
        "lista = [test11]\r\n",
        "\r\n",
        "\r\n",
        "lista = tokenizer.texts_to_sequences(lista)\r\n",
        "lista = pad_sequences(lista, padding='post', maxlen=maxlen)\r\n",
        "tst = model.predict(np.array( lista ))\r\n",
        "\r\n",
        "pred_name = np.argmax(tst)\r\n",
        "print(pred_name)\r\n",
        "#tst\r\n",
        "if(pred_name == 1):\r\n",
        "  print(\"it is non offensive\")\r\n",
        "\r\n",
        "elif(pred_name ==0 ):\r\n",
        "  print(\"it is offensive\")\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOJhpXHoVITS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}