{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ToxicComments.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1_sqDV99oWum2XDB-5cZvOPDbCiqNUJH1",
      "authorship_tag": "ABX9TyPrMM7DmXSR037wNkzbx8Qm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xanasa14/MLImplementations/blob/master/ToxicComments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0SFzyZpYcm6"
      },
      "source": [
        "from numpy import array\r\n",
        "from keras.preprocessing.text import one_hot\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers.core import Activation, Dropout, Dense\r\n",
        "from keras.layers import Flatten, LSTM\r\n",
        "from keras.layers import GlobalMaxPooling1D\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.layers import Input\r\n",
        "from keras.layers.merge import Concatenate\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import nltk \r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "#98.08%"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsuFB3oKYcir"
      },
      "source": [
        "\r\n",
        "#toxic_comments = pd.read_csv(\"/content/train.csv\",engine='python',encoding='utf-8')\r\n",
        "toxic_comments = pd.read_csv(\"/content/drive/MyDrive/toxicCommentz/train.csv\")\r\n",
        "\r\n",
        "toxic_comments.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnw6Z4ZKX0Kj"
      },
      "source": [
        "print(toxic_comments['comment_text'].head())\r\n",
        "print(toxic_comments.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo8Ec-uuZNFt"
      },
      "source": [
        "filter = toxic_comments[\"comment_text\"] != \"\"\r\n",
        "toxic_comments = toxic_comments[filter]\r\n",
        "toxic_comments = toxic_comments.dropna()\r\n",
        "print(toxic_comments[\"comment_text\"][168])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Yhbu89L286z"
      },
      "source": [
        "counter = 0\r\n",
        "offensive = []\r\n",
        "nonOffensive = []\r\n",
        "for i in range(len(toxic_comments)):\r\n",
        "  #if (i <= 500):\r\n",
        "  if (int(toxic_comments[\"toxic\"][i]) == 1 or int(toxic_comments[\"severe_toxic\"][i]) == 1 or \r\n",
        "      int(toxic_comments[\"obscene\"][i]) == 1 or int(toxic_comments[\"threat\"][i]) == 1 or\r\n",
        "      int(toxic_comments[\"insult\"][i]) == 1 or int(toxic_comments[\"identity_hate\"][i]) == 1):\r\n",
        "    #print(\"Toxic:\" + str(toxic_comments[\"toxic\"][i]))\r\n",
        "    #print(\"Severe_toxic:\" + str(toxic_comments[\"severe_toxic\"][i]))\r\n",
        "    #print(\"Obscene:\" + str(toxic_comments[\"obscene\"][i]))\r\n",
        "    #print(\"Threat:\" + str(toxic_comments[\"threat\"][i]))\r\n",
        "    #print(\"Insult:\" + str(toxic_comments[\"insult\"][i]))\r\n",
        "    #print(\"Identity_hate:\" + str(toxic_comments[\"identity_hate\"][i]))\r\n",
        "    offensive.append(1)\r\n",
        "    nonOffensive.append(0)\r\n",
        "  else: \r\n",
        "    nonOffensive.append(1)\r\n",
        "    offensive.append(0)\r\n",
        "\r\n",
        "# Declare a list that is to be converted into a column \r\n",
        "\r\n",
        "  \r\n",
        "# Using 'Address' as the column name \r\n",
        "# and equating it to the list \r\n",
        "toxic_comments['offensive'] = offensive\r\n",
        "toxic_comments['nonOffensive'] = nonOffensive\r\n",
        "\r\n",
        "\r\n",
        "#toxic_comments = toxic_comments[toxic_comments.offensive == 1]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbHflVeya3y4"
      },
      "source": [
        "\r\n",
        "\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][96]))\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][960]))\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][9600]))\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][96000]))\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][77]))\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][777]))\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][7777]))\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][123]))\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][1234]))\r\n",
        "print(\"comment_text:\" + str(toxic_comments[\"comment_text\"][12345]))\r\n",
        "\r\n",
        "print(\"Toxic:\" + str(toxic_comments[\"toxic\"][0]))\r\n",
        "print(\"Severe_toxic:\" + str(toxic_comments[\"severe_toxic\"][0]))\r\n",
        "print(\"Obscene:\" + str(toxic_comments[\"obscene\"][0]))\r\n",
        "print(\"Threat:\" + str(toxic_comments[\"threat\"][0]))\r\n",
        "print(\"Insult:\" + str(toxic_comments[\"insult\"][0]))\r\n",
        "print(\"Identity_hate:\" + str(toxic_comments[\"identity_hate\"][0]))\r\n",
        "\r\n",
        "\r\n",
        "len(toxic_comments)\r\n",
        "print(\"Offensive :\" + str (toxic_comments['offensive'][0]))\r\n",
        "print(\"Non-offensive :\" + str (toxic_comments['nonOffensive'][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr1jrdxra5LP"
      },
      "source": [
        "#toxic_comments_labels = toxic_comments[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]]\r\n",
        "toxic_comments_labels = toxic_comments[[\"offensive\", \"nonOffensive\"]]\r\n",
        "\r\n",
        "toxic_comments_OffensiveOrNot = toxic_comments[[\"comment_text\",\"offensive\", \"nonOffensive\"]]\r\n",
        "print(toxic_comments_OffensiveOrNot[\"offensive\"][168])\r\n",
        "print(toxic_comments_OffensiveOrNot[\"comment_text\"][168])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u96GFxTPbAW1"
      },
      "source": [
        "fig_size = plt.rcParams[\"figure.figsize\"]\r\n",
        "fig_size[0] = 10\r\n",
        "fig_size[1] = 8\r\n",
        "plt.rcParams[\"figure.figsize\"] = fig_size\r\n",
        "\r\n",
        "toxic_comments_labels.sum(axis=0).plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEiKGG2gx7M3"
      },
      "source": [
        "\r\n",
        "import spacy\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\r\n",
        "stop_words = stopwords.words('english')\r\n",
        "from nltk.stem import WordNetLemmatizer \r\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) \r\n",
        "import string  \r\n",
        "def loweringText(text):\r\n",
        "    text = text.lower()\r\n",
        "    return text\r\n",
        "#Removing punctuationes and those characters\r\n",
        "def remove_punctuations(text):\r\n",
        "  for punctuation in string.punctuation:\r\n",
        "      text = text.replace(punctuation, '')\r\n",
        "  return text\r\n",
        "# Lowering Text from DataFrame\r\n",
        "\r\n",
        "#Remove stopwords : words that do not put as much significance or impact as other words in the description\r\n",
        "def remove_StopWords(text):\r\n",
        "  line = text.split()\r\n",
        "  text = \"\"\r\n",
        "  for word in line:\r\n",
        "    if(word not in stop_words):\r\n",
        "      text += word\r\n",
        "      text += \" \"\r\n",
        "  return text\r\n",
        "#Implementing lemmatization words by word\r\n",
        "def lemmatize(text):\r\n",
        "  line = text.split()\r\n",
        "  txt = \"\"\r\n",
        "  for word in line:\r\n",
        "    doc = nlp(word)\r\n",
        "    for token in doc:\r\n",
        "      txt += token.lemma_\r\n",
        "      txt += \" \"\r\n",
        "  return txt\r\n",
        "import re\r\n",
        "\r\n",
        "def remove_abb(text):\r\n",
        "  for line in text:\r\n",
        "    #Either use:\r\n",
        "    text = re.sub('http://\\S+|https://\\S+', '', text)\r\n",
        "    text = re.sub('http[s]?://\\S+', '', text)\r\n",
        "\r\n",
        "    text = text.replace(\"he's\", \"he is\")\r\n",
        "    text = text.replace(\"there's\", \"there is\")\r\n",
        "    text = text.replace(\"we're\", \"we are\")\r\n",
        "    text = text.replace(\"that's\", \"that is\")\r\n",
        "    text = text.replace(\"won't\", \"will not\")\r\n",
        "    text = text.replace(\"they're\", \"they are\")\r\n",
        "    text = text.replace(\"can't\", \"cannot\")\r\n",
        "    text = text.replace(\"wasn't\", \"was not\")\r\n",
        "    text = text.replace(\"don\\x89Ûªt\", \"do not\")\r\n",
        "    text = text.replace(\"aren't\", \"are not\")\r\n",
        "    text = text.replace(\"isn't\", \"is not\")\r\n",
        "    text = text.replace(\"what's\", \"what is\")\r\n",
        "    text = text.replace(\"haven't\", \"have not\")\r\n",
        "    text = text.replace(\"hasn't\", \"has not\")\r\n",
        "    text = text.replace(\"there's\", \"there is\")\r\n",
        "    text = text.replace(\"he's\", \"he is\")\r\n",
        "    text = text.replace(\"it's\", \"it is\")\r\n",
        "    text = text.replace(\"you're\", \"you are\")\r\n",
        "    text = text.replace(\"i'm\", \"i am\")\r\n",
        "    text = text.replace(\"shouldn't\", \"should not\")\r\n",
        "    text = text.replace(\"wouldn't\", \"would not\")\r\n",
        "    text = text.replace(\"I\\x89Ûªm\", \"i am\")\r\n",
        "    text = text.replace(\"here's\", \"here is\")\r\n",
        "    text = text.replace(\"you've\", \"you have\")\r\n",
        "    text = text.replace(\"you\\x89Ûªve\", \"you have\")\r\n",
        "    text = text.replace(\"couldn't\", \"could not\")\r\n",
        "    text = text.replace(\"we've\", \"we have\")\r\n",
        "    text = text.replace(\"it\\x89Ûªs\", \"it is\")\r\n",
        "    text = text.replace(\"doesn\\x89Ûªt\", \"does not\")\r\n",
        "    text = text.replace(\"it\\x89Ûªs\", \"it is\")\r\n",
        "    text = text.replace(\"here\\x89Ûªs\", \"here is\")\r\n",
        "    text = text.replace(\".jpeg\", \" \")\r\n",
        "    text = text.replace(\".jpg\", \" \")\r\n",
        "    text = text.replace(\".png\", \" \")\r\n",
        "    text = text.replace(\"'s\", \" is \")\r\n",
        "    text = text.replace(\"→\", \" is \")  \r\n",
        "    text = text.replace(\"\\n\", \" \")\r\n",
        "    text = text.replace(\"trácht\", \" \")\r\n",
        "    text = text.replace(\"piece\", \" \")\r\n",
        "    text = text.replace(\"didn't\", \"did not\")\r\n",
        "    text = text.replace(\"who's\", \"who is\")\r\n",
        "    text = text.replace(\"i\\x89Ûªve\", \"i have\")\r\n",
        "    text = text.replace(\"y'all\", \"you all\")\r\n",
        "    text = text.replace(\"can\\x89Ûªt\", \"cannot\")\r\n",
        "    text = text.replace(\"would've\", \"would have\")\r\n",
        "    text = text.replace(\"it'll\", \"it will\")\r\n",
        "    text = text.replace(\"we'll\", \"we will\")\r\n",
        "    text = text.replace(\"wouldn\\x89Ûªt\", \"would not\")\r\n",
        "    text = text.replace(\"we've\", \"we have\")\r\n",
        "    text = text.replace(\"he'll\", \"he will\")\r\n",
        "    text = text.replace(\"weren't\", \"were not\")\r\n",
        "    text = text.replace(\"they'll\", \"they will\")\r\n",
        "    text = text.replace(\"they'd\", \"they would\")\r\n",
        "    text = text.replace(\"that\\x89Ûªs\", \"that is\")\r\n",
        "    text = text.replace(\"they've\", \"they have\")\r\n",
        "    text = text.replace(\"i'd\", \"i would\")\r\n",
        "    text = text.replace(\"donå«t\", \"do not\")\r\n",
        "    text = text.replace(\"should've\", \"should have\")\r\n",
        "    text = text.replace(\"you\\x89Ûªre\", \"you are\")\r\n",
        "    text = text.replace(\"where's\", \"where is\")\r\n",
        "    text = text.replace(\"don\\x89Ûªt\", \"do not\")\r\n",
        "    text = text.replace(\"we'd\", \"we would\")\r\n",
        "    text = text.replace(\"i'll\", \"i will\")\r\n",
        "    text = text.replace(\"weren't\", \"were not\")\r\n",
        "    text = text.replace(\"can\\x89Ûªt\", \"cannot\")\r\n",
        "    text = text.replace(\"you\\x89Ûªll\", \"you will\")\r\n",
        "    text = text.replace(\"I\\x89Ûªd\", \"i would\")\r\n",
        "    text = text.replace(\"let's\", \"let us\")\r\n",
        "    text = text.replace(\"it's\", \"it is\")\r\n",
        "    text = text.replace(\"don't\", \"do not\")\r\n",
        "    text = text.replace(\"you're\", \"you are\")\r\n",
        "    text = text.replace(\"i've\", \"i have\")\r\n",
        "    text = text.replace(\"ain't\", \"am not\")\r\n",
        "    text = text.replace(\"doesn't\", \"does not\")\r\n",
        "    text = text.replace(\"i'd\", \"i would\")\r\n",
        "    text = text.replace(\"didn't\", \"did not\")\r\n",
        "    text = text.replace(\"you'll\", \"you will\")\r\n",
        "    text = text.replace(\"let's\", \"let us\")\r\n",
        "    text = text.replace(\"youve\", \"you have\")\r\n",
        "    text = text.replace(\"could've\", \"could have\")\r\n",
        "    text = text.replace(\"Haven't\", \"have not\")\r\n",
        "    text = text.replace(\"you'd\", \"you would\")\r\n",
        "    text = text.replace(\"it's\", \"it is\")\r\n",
        "    text = text.replace(\"like \", \" \")\r\n",
        "    text = text.replace(\"wikipedia \", \" \")\r\n",
        "    text = text.replace(\"article \", \" \")\r\n",
        "    text = text.replace(\"articles \", \" \")\r\n",
        "    text = text.replace(\"know \", \" \")\r\n",
        "    text = text.replace(\"want \", \" \")\r\n",
        "    text = text.replace(\"page \", \" \")\r\n",
        "    text = text.replace(\"wiki \", \" \")\r\n",
        "    text = text.replace(\"im \", \" \")\r\n",
        "    text = text.replace(\"still \", \" \")\r\n",
        "    text = text.replace(\"hi \", \" \")\r\n",
        "    text = text.replace(\"talk \", \" \")\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        " \r\n",
        "    text = text.replace(\"admin \", \" \")\r\n",
        "    text = text.replace(\" — \", \" \")\r\n",
        "\r\n",
        "    text = text.replace(\"「\", \" \")\r\n",
        "    text = text.replace(\"」\", \" \")\r\n",
        "    text = text.replace(\"¤\", \" \")\r\n",
        "    text = text.replace(\"¢\", \" \")\r\n",
        "    text = text.replace(\" – \", \" \")\r\n",
        "    text = text.replace(\" a \", \" \")\r\n",
        "    text = text.replace(\" b \", \" \")\r\n",
        "    text = text.replace(\" c \", \" \")\r\n",
        "    text = text.replace(\" d \", \" \")\r\n",
        "    text = text.replace(\" e \", \" \")\r\n",
        "    text = text.replace(\" f \", \" \")\r\n",
        "    text = text.replace(\" g \", \" \")\r\n",
        "    text = text.replace(\" h \", \" \")\r\n",
        "    text = text.replace(\" i \", \" \")\r\n",
        "    text = text.replace(\" j \", \" \")\r\n",
        "    text = text.replace(\" k \", \" \")\r\n",
        "    text = text.replace(\" l \", \" \")\r\n",
        "    text = text.replace(\" m \", \" \")\r\n",
        "    text = text.replace(\" n \", \" \")\r\n",
        "    text = text.replace(\" o \", \" \")\r\n",
        "    text = text.replace(\" p \", \" \")\r\n",
        "    text = text.replace(\" q \", \" \")\r\n",
        "    text = text.replace(\" r \", \" \")\r\n",
        "    text = text.replace(\" s \", \" \")\r\n",
        "    text = text.replace(\" t \", \" \")\r\n",
        "    text = text.replace(\" u \", \" \")\r\n",
        "    text = text.replace(\" v \", \" \")\r\n",
        "    text = text.replace(\" w \", \" \")\r\n",
        "    text = text.replace(\" x \", \" \")\r\n",
        "    text = text.replace(\" y \", \" \")\r\n",
        "    text = text.replace(\" z \", \" \")\r\n",
        "    text = text.replace(\"wp\", \" \")\r\n",
        "    \r\n",
        "\r\n",
        "    text = text.replace(\"@\", \" \")\r\n",
        "    text = text.replace(\"?\", \" \")\r\n",
        "    text = text.replace(\"!\",\" \")\r\n",
        "    text = text.replace(\"\\\"\",\"\")\r\n",
        "    text = text.replace(\"(\",\"\")\r\n",
        "    text = text.replace(\")\",\"\")\r\n",
        "    text = text.replace(\"+\", \" \") \r\n",
        "    text = text.replace(\"#\", \" \")\r\n",
        "    text = text.replace(\"}\",\" \")\r\n",
        "    text = text.replace(\"{\",\" \")\r\n",
        "    text = text.replace(\"&\",\" \")\r\n",
        "    text = text.replace(\";\",\" \")\r\n",
        "    text = text.replace(\"%\",\" \")\r\n",
        "    text = text.replace(\":\", \" \")\r\n",
        "    text = text.replace(\"$\",\" \")\r\n",
        "    text = text.replace(\"|\", \" \")\r\n",
        "    text = text.replace(\".\", \" \")\r\n",
        "    text = text.replace(\"·\",\" \")\r\n",
        "    text = text.replace(\",\",\" \")\r\n",
        "    text = text.replace(\" -\",\" \")\r\n",
        "    text = text.replace(\"~\",\" \")\r\n",
        "    text = text.replace(\"1\", \" \") \r\n",
        "    text = text.replace(\"2\", \" \")\r\n",
        "    text = text.replace(\"3\", \" \")\r\n",
        "    text = text.replace(\"4\", \" \")\r\n",
        "    text = text.replace(\"5\",\" \")\r\n",
        "    text = text.replace(\"6\",\" \") \r\n",
        "    text = text.replace(\"7\", \" \")\r\n",
        "    text = text.replace(\"8\", \" \")\r\n",
        "    text = text.replace(\"9\",\" \")\r\n",
        "    text = text.replace(\"0\", \" \")\r\n",
        "    text = text.replace(\"%\", \" \")\r\n",
        "    text = text.replace(\"'\", \" \")\r\n",
        "    text = text.replace(\"/\", \" \")\r\n",
        "    text = text.replace(\"<\", \" \")\r\n",
        "    text = text.replace(\">\", \" \")\r\n",
        "    text = text.replace(\"[\", \" \")\r\n",
        "    text = text.replace(\"]\", \" \")\r\n",
        "    text = text.replace(\"_\",\" \")\r\n",
        "    text = text.replace(\"^\",\" \")\r\n",
        "    text = text.replace(\"`\",\" \")\r\n",
        "    text = text.replace(\"=\",\" \")\r\n",
        "    text = text.replace(\"\\\\\",\" \")\r\n",
        "    text = text.replace(\"*\", \" \")\r\n",
        "    text = text.replace(\"- \", \" \")\r\n",
        "    text = text.replace(\"•\", \" \")  \r\n",
        "    text = text.replace(\"   \", \" \")\r\n",
        "    text = text.replace(\"  \", \" \")\r\n",
        "    return text\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgVWk3eW6H2X"
      },
      "source": [
        "#toxic_comments[\"comment_text_X\"] = toxic_comments['comment_text'].apply(remove_punctuations).apply(loweringText).apply(remove_StopWords).apply(lemmatize)\r\n",
        "#FIRST LOWER CASE EVERYTHING \r\n",
        "toxic_comments[\"comment_text\"] = toxic_comments['comment_text'].apply(loweringText)\r\n",
        "print(\"Lowering the Text \")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCmMISDeBqAt"
      },
      "source": [
        "\r\n",
        "toxic_comments[\"comment_text\"] = toxic_comments['comment_text'].apply(remove_abb)\r\n",
        "print(\"removed remove_abb \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5BnkuBrCCz4"
      },
      "source": [
        "toxic_comments[\"comment_text\"] = toxic_comments['comment_text'].apply(remove_StopWords)\r\n",
        "print(\"removed remove_StopWords \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oztVMIXaUMf"
      },
      "source": [
        "#TAKES TOO MUCH TIME DUE TO THE HUGE DATASET\r\n",
        "#toxic_comments[\"comment_text\"] = toxic_comments['comment_text'].apply(lemmatize)\r\n",
        "#print(\"removed lemmatize \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vzpBQMNbEfI"
      },
      "source": [
        "def preprocess_text(sen):\r\n",
        "    # Remove punctuations and numbers\r\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\r\n",
        "\r\n",
        "    # Single character removal\r\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\r\n",
        "\r\n",
        "    # Removing multiple spaces\r\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\r\n",
        "\r\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOoGpkabbHvz"
      },
      "source": [
        "z = []\r\n",
        "#toxic_comments_labels = toxic_comments[[\"offensive\"]]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "sentences = list(toxic_comments[\"comment_text\"])\r\n",
        "for sen in sentences:\r\n",
        "    z.append(preprocess_text(sen))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "y = toxic_comments_labels.values\r\n",
        "X = toxic_comments[\"comment_text\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQwBQF3xvgRF"
      },
      "source": [
        "print(sentences [96])\r\n",
        "print(sentences [960])\r\n",
        "print(sentences [9600])\r\n",
        "print(sentences [96000])\r\n",
        "print(sentences [777])\r\n",
        "print(sentences [77])\r\n",
        "print(sentences [555])\r\n",
        "print(sentences [666])\r\n",
        "print(sentences [888])\r\n",
        "print(sentences [999])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj1-4XcubJIX"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,\n",
        "                                                    random_state=42, stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5tEPYFbRcM9"
      },
      "source": [
        "print(X_train[0])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_U5wPK_bU8z"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\r\n",
        "tokenizer.fit_on_texts(X_train)\r\n",
        "\r\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\r\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\r\n",
        "\r\n",
        "vocab_size = len(tokenizer.word_index) + 1\r\n",
        "\r\n",
        "maxlen = 200\r\n",
        "\r\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\r\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTBFJK6lbWoD"
      },
      "source": [
        "from numpy import array\r\n",
        "from numpy import asarray\r\n",
        "from numpy import zeros\r\n",
        "\r\n",
        "embeddings_dictionary = dict()\r\n",
        "\r\n",
        "#glove_file = open('/content/drive/My Drive/Colab Datasets/glove.6B.100d.txt', encoding=\"utf8\")\r\n",
        "glove_file = open('/content/drive/MyDrive/gloves/glove.6B.100d.txt', encoding=\"utf8\")\r\n",
        "\r\n",
        "for line in glove_file:\r\n",
        "    records = line.split()\r\n",
        "    word = records[0]\r\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\r\n",
        "    embeddings_dictionary[word] = vector_dimensions\r\n",
        "glove_file.close()\r\n",
        "\r\n",
        "embedding_matrix = zeros((vocab_size, 100))\r\n",
        "for word, index in tokenizer.word_index.items():\r\n",
        "    embedding_vector = embeddings_dictionary.get(word)\r\n",
        "    if embedding_vector is not None:\r\n",
        "        embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fi8MCl7bg7q"
      },
      "source": [
        "deep_inputs = Input(shape=(maxlen,))\r\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(deep_inputs)\r\n",
        "LSTM_Layer_1 = LSTM(128)(embedding_layer)\r\n",
        "dense_layer_1 = Dense(2, activation='sigmoid')(LSTM_Layer_1)\r\n",
        "model = Model(inputs=deep_inputs, outputs=dense_layer_1)\r\n",
        "\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phe5qujWcSFX"
      },
      "source": [
        "print(model.summary())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc3z61lFcV-n"
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size=128, epochs=15, verbose=1, validation_split=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSyq9ZT-cZ9E"
      },
      "source": [
        "\r\n",
        "text_model = history\r\n",
        "score = history.model.evaluate(X_test, y_test, verbose=1)\r\n",
        "\r\n",
        "print(\"Test Score:\", score[0])\r\n",
        "print(\"Test Accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV-YwGi_CLew"
      },
      "source": [
        "\r\n",
        "print(X_test.shape)\r\n",
        "print(y_test.shape)\r\n",
        "\r\n",
        "print(score)\r\n",
        "toxic_comments_labels.values\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKzGt7nCqIMJ"
      },
      "source": [
        "history.model.predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY8LpPC-l2fm"
      },
      "source": [
        "model.save('/content/drive/MyDrive/toxicCommentz/my_model.h5')\r\n",
        "# LOAD \r\n",
        "import keras\r\n",
        "model = keras.models.load_model('/content/drive/MyDrive/toxicCommentz/my_model.h5')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pX5APvWtKew"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlDNgxxIB62I"
      },
      "source": [
        "y_pred = history.model.predict(X_test)\r\n",
        "print(y_pred)\r\n",
        "print(y_test)\r\n",
        "print(y_pred.shape)\r\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-5Khb5pc_oF"
      },
      "source": [
        "toxic_comments.iloc[24579][\"comment_text\"]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt2HI-xdaOa_"
      },
      "source": [
        "print(len(y_pred))\r\n",
        "print(len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoXUNjG3p_q_"
      },
      "source": [
        "\r\n",
        "\r\n",
        "import sklearn.metrics as metrics\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "target_names= [\"Offensive\" , \"NonOffensive\"]\r\n",
        "print(classification_report(np.argmax(y_pred, axis=1), np.argmax(y_test, axis=1), target_names=target_names))\r\n",
        "\r\n",
        "\r\n",
        "matrix = metrics.confusion_matrix(y_pred.argmax(axis=1), y_test.argmax(axis=1),)\r\n",
        "# 151\r\n",
        "# 63678\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n2-36qn-uRJ"
      },
      "source": [
        "matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d6SFcbB_TPW"
      },
      "source": [
        "print(y_pred.shape)\r\n",
        "y_pred2 = y_pred\r\n",
        "y_test2 = y_test\r\n",
        "\r\n",
        "Size = len (y_test2)\r\n",
        "\r\n",
        "y_test2 = y_test2.reshape((Size,-1))\r\n",
        "y_pred2 = y_pred2.reshape((Size,-1))\r\n",
        "\r\n",
        "\r\n",
        "print(y_pred2.shape)\r\n",
        "print(y_test2.shape)\r\n",
        "\r\n",
        "matrix = metrics.confusion_matrix(y_pred2.argmax(axis=1), y_test2.argmax(axis=1))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se5Hr8L5Wlm8"
      },
      "source": [
        "matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCx6dvYxNme0"
      },
      "source": [
        "CR = metrics.accuracy_score(y_pred2.argmax(axis=1), y_test2.argmax(axis=1))\r\n",
        "print(CR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0R7ynutsGSt"
      },
      "source": [
        "strin = \"Link to http://www.langandlit.ualberta.ca/Fall2004/SteigelBainbridge.html mentions this a bit - he stood as an election candidate for Respect.\" \r\n",
        "strin = strin.replace(\"http:\\/\\/[A-Za-z0-9.\\/]+html\",\"\")\r\n",
        "strin\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEQC-zir9yJL"
      },
      "source": [
        "#Define the text from which you want to replace the url with \"\".\r\n",
        "text ='''The link to this post is http://www.langandlit.ualberta.ca/Fall2004/SteigelBainbridge.html'''\r\n",
        "\r\n",
        "import re\r\n",
        "#Either use:\r\n",
        "re.sub('http://\\S+|https://\\S+', '', text)\r\n",
        "#OR \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzWRfxsbM8m4"
      },
      "source": [
        "test10 = \"gift cards downloads december possible use gift cards purchase mp even though gift card faq claims yes amazon com gift cards used buy amazon mp unbox downloads strictly speaking false statement able pay orders gift card time one person already removed line reverted good way go providing verification \"\r\n",
        "#PREDICTING ONE \r\n",
        "\r\n",
        "test11 = \"still\"\r\n",
        "\r\n",
        "\r\n",
        "lista = [test11]\r\n",
        "\r\n",
        "\r\n",
        "lista = tokenizer.texts_to_sequences(lista)\r\n",
        "lista = pad_sequences(lista, padding='post', maxlen=maxlen)\r\n",
        "tst = model.predict(np.array( lista ))\r\n",
        "\r\n",
        "pred_name = np.argmax(tst)\r\n",
        "print(pred_name)\r\n",
        "#tst\r\n",
        "if(pred_name == 1):\r\n",
        "  print(\"it is Non-offensive\")\r\n",
        "\r\n",
        "elif(pred_name ==0 ):\r\n",
        "  print(\"it is Offensive\")\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDvxySJ4osqm"
      },
      "source": [
        "#WordCloud Visualizations\r\n",
        "#Method for creating wordclouds\r\n",
        "from wordcloud import WordCloud,STOPWORDS\r\n",
        "\r\n",
        "from PIL import Image\r\n",
        "def display_cloud(data,label):\r\n",
        "    plt.subplots(figsize=(10,10))\r\n",
        "    text = toxic_comments[toxic_comments[label]==1][\"comment_text\"].tolist()\r\n",
        "    wc = WordCloud(\r\n",
        "                          stopwords=STOPWORDS,\r\n",
        "                          background_color='black',\r\n",
        "                          collocations=False,\r\n",
        "                          width=1800,\r\n",
        "                          height=800,\r\n",
        "                         )\r\n",
        "    wc.generate(' '.join(text))\r\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\r\n",
        "    plt.axis('off')\r\n",
        "    plt.title(label,fontsize=36)\r\n",
        "    plt.show()\r\n",
        "    \r\n",
        "display_cloud(toxic_comments,\"offensive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2AUH0tMpfSn"
      },
      "source": [
        "#WordCloud Visualizations\r\n",
        "#Method for creating wordclouds\r\n",
        "from wordcloud import WordCloud,STOPWORDS\r\n",
        "\r\n",
        "from PIL import Image\r\n",
        "def display_cloud(data,label):\r\n",
        "    plt.subplots(figsize=(10,10))\r\n",
        "    text = toxic_comments[toxic_comments[label]==1][\"comment_text\"].tolist()\r\n",
        "    wc = WordCloud(\r\n",
        "                          stopwords=STOPWORDS,\r\n",
        "                          background_color='black',\r\n",
        "                          collocations=False,\r\n",
        "                          width=1800,\r\n",
        "                          height=800,\r\n",
        "                         )\r\n",
        "    wc.generate(' '.join(text))\r\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\r\n",
        "    plt.axis('off')\r\n",
        "    plt.title(label,fontsize=36)\r\n",
        "    plt.show()\r\n",
        "    \r\n",
        "display_cloud(toxic_comments,\"nonOffensive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V37RRLqRp4nH"
      },
      "source": [
        "from collections import Counter\r\n",
        "text = toxic_comments[toxic_comments[\"offensive\"]==1][\"comment_text\"].tolist()\r\n",
        "print(type(text))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfzLkyaXsFHD"
      },
      "source": [
        "from collections import Counter\r\n",
        "print(type(text))\r\n",
        "\r\n",
        "print(\"Offensive words \")\r\n",
        "Counter(\" \".join(toxic_comments[toxic_comments[\"offensive\"]==1][\"comment_text\"]).split()).most_common(20)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G6gYKOJtAft"
      },
      "source": [
        "from collections import Counter\r\n",
        "print(type(text))\r\n",
        "\r\n",
        "print(\"Non-Offensive words \")\r\n",
        "Counter(\" \".join(toxic_comments[toxic_comments[\"nonOffensive\"]==1][\"comment_text\"]).split()).most_common(10)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdSK4A7QtZGv"
      },
      "source": [
        "!pip install eli5\r\n",
        "\r\n",
        "from IPython.display import display, HTML\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bP9f4I8q4Qh"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7ffo1-sp03l"
      },
      "source": [
        "stop_words_ = set(stopwords.words('english'))\n",
        "wn = WordNetLemmatizer()\n",
        "\n",
        "my_sw = ['rt', 'ht', 'fb', 'amp', 'gt']\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "def black_txt(token):\n",
        "  if token == 'u':\n",
        "    token = 'you'\n",
        "  return  token not in stop_words_ and token not in list(string.punctuation) and token not in my_sw\n",
        "\n",
        "def cleaner(word):\n",
        "  #Remove links\n",
        "  word = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', \n",
        "                '', word, flags=re.MULTILINE)\n",
        "  #Decontracted words\n",
        "  word = decontracted(word)\n",
        "  #Remove users mentions\n",
        "  word = re.sub(r'(@[^\\s]*)', \"\", word)\n",
        "  word = re.sub('[\\W]', ' ', word)\n",
        "  #Lemmatized\n",
        "  list_word_clean = []\n",
        "  for w1 in word.split(\" \"):\n",
        "    if  black_txt(w1.lower()):\n",
        "      word_lemma =  wn.lemmatize(w1,  pos=\"v\")\n",
        "      list_word_clean.append(word_lemma)\n",
        "\n",
        "  #Cleaning, lowering and remove whitespaces\n",
        "  word = \" \".join(list_word_clean)\n",
        "  word = re.sub('[^a-zA-Z]', ' ', word)\n",
        "  return word.lower().strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Oq7IQ-Dp-Fz"
      },
      "source": [
        "toxic_comments.iloc[0]['comment_text']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6El7dHwWqkLl"
      },
      "source": [
        "cleaner(toxic_comments.iloc[0]['comment_text'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MADWpNYk0AiN"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,\n",
        "                                                    random_state=42, stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5Z-AcCi67M-"
      },
      "source": [
        "print(type(X))\n",
        "print(type(y))\n",
        "\n",
        "print(type(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp2fYEHhqHeu"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
        "\n",
        "def _get_sequences(texts):\n",
        "  seqs = tokenizer.texts_to_sequences(texts)\n",
        "  return pad_sequences(seqs, maxlen=input_length, value=0)\n",
        "\n",
        "def _preprocess(texts):\n",
        "  return [cleaner(x) for x in texts]\n",
        "\n",
        "def fit( X, y):\n",
        "  '''Fit the vocabulary and the model.\n",
        "      :params: X: list of texts. y: labels.\n",
        "  '''\n",
        "  tokenizer.fit_on_texts(X)\n",
        "  tokenizer.word_index = {e: i for e,i in tokenizer.word_index.items() if i <= max_words}\n",
        "  tokenizer.word_index[tokenizer.oov_token] = max_words + 1\n",
        "  seqs = _get_sequences(_preprocess(X))\n",
        "  model.fit([seqs ], y, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "def predict_proba(X, y=None):\n",
        "  seqs = _get_sequences(_preprocess(X))\n",
        "  print(\"***\")\n",
        "  print(seqs)\n",
        "  print(type(seqs))\n",
        "  print(\"***\")\n",
        "  print(\"!!!!!!\")\n",
        "  print(model.predict(seqs).shape)\n",
        "  print(\"!!!!!!\")\n",
        "  return model.predict(seqs)\n",
        "\n",
        "def predict( X, y=None):\n",
        "  return np.argmax(predict_proba(X), axis=1)\n",
        "  \n",
        "\n",
        "def score( X, y):\n",
        "  y_pred = predict(X)\n",
        "  print(y_pred)\n",
        "  return accuracy_score(np.argmax(y, axis=1), y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQpVbeCilf37"
      },
      "source": [
        "import eli5\n",
        "from eli5.lime import TextExplainer\n",
        "input_length=200\n",
        "\n",
        "for idx in X_test.index[190:193]:\n",
        "  print(X_test[idx])\n",
        "  te = TextExplainer(random_state=42)\n",
        "  print(\"XXXXXX: \" , predict_proba)\n",
        "  te.fit(cleaner(X_test[idx]), predict_proba, )\n",
        "  print(\"Real Class:\",  [\"Nonoffensive\" if x == 0 else \"Offensive\" for x in [toxic_comments.iloc[idx]['Offensive']]])\n",
        "  print(\"Text uncleaned tweet:\", toxic_comments.iloc[idx]['text_comment'])\n",
        "  print(\"ELI5 Predicted Class:\")\n",
        "  HTML(display((te.show_prediction(target_names=[ 'Nonoffensive Toxic','Offensive',]))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFWdjiAVl3ou"
      },
      "source": [
        "X_test[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izqG6M0hmA-Q"
      },
      "source": [
        "X_test[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdFQQVGGtFcp"
      },
      "source": [
        "def predict_proba(X, y=None):\n",
        "  #print(X)\n",
        "  seqs = _get_sequences(_preprocess(X))\n",
        "  print(seqs)\n",
        "  return model.predict(seqs)\n",
        "def predict( X, y=None):\n",
        "\n",
        "  return np.argmax(predict_proba(X), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOPWv5bvRkAu"
      },
      "source": [
        "predict_proba(X_test[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFmMPBsdSZ6g"
      },
      "source": [
        "score(X_test, y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQwUF6zgbbhL"
      },
      "source": [
        "for idx in X_test.index[2:3]:\n",
        "  print(idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tAxJptc4tlC"
      },
      "source": [
        "print(type(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skdDGAU76JRM"
      },
      "source": [
        "for idx in X_test.index[190:193]:\n",
        "  print(cleaner(X_test[idx]))\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a1Qh8pY8gXI"
      },
      "source": [
        "#te.samples_\n",
        "te.show_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN8AgpYaBJnJ"
      },
      "source": [
        "print(len(predict_proba(te.samples_).sum(axis=1)))\n",
        "predict_proba(te.samples_).shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPIJHD3bBrNJ"
      },
      "source": [
        "Size = predict_proba(te.samples_).sum(axis=1)\n",
        "print(predict_proba(te.samples_).shape)\n",
        "y_pred3 = predict_proba(te.samples_).sum(axis=1).shape\n",
        "#y_pred3 = predict_proba(te.samples_).sum(axis=1).shape\n",
        "#y_pred3 = predict_proba(te.samples_).reshape(2,-1)\n",
        "\n",
        "#y_pred3.shape\n",
        "np.argmax(predict_proba(X_test[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kmiImueG8u7"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ipyIjVEMWaR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}